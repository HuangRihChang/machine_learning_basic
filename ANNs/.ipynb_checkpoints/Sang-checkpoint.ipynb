{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BT06: Titanic\n",
    "\n",
    "Thái Thanh Sang - 1412454\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tổng thể\n",
    "\n",
    "**Cách làm bài**\n",
    "\n",
    "Bạn sẽ làm trực tiếp trên file notebook này; trong file, mình đã để từ `TODO` để cho biết những chỗ mà bạn cần phải làm (trong đó, `TODO` đầu tiên là bạn phải ghi họ tên và MSSV vào phần đầu của file). Trong khi làm bài, thường xuyên `Ctrl + S` để lưu lại bài làm của bạn, tránh mất mát thông tin.\n",
    "\n",
    "Nên nhớ mục tiêu chính ở đây là *học, học một cách chân thật*. Bạn có thể thảo luận ý tưởng với bạn khác cũng như là tham khảo các tài liệu, nhưng *code và bài làm phải là của bạn, dựa trên sự hiểu của bạn*. Nếu vi phạm thì sẽ bị 0 điểm cho toàn bộ môn học.\n",
    "\n",
    "**Cách nộp bài**\n",
    "\n",
    "Trước khi nộp bài, bạn chọn `Kernel` -> `Restart & Run All` (restart python và chạy tất cả các cell), rồi kiểm tra xem có bị lỗi gì không.\n",
    "\n",
    "Sau đó, trong thư mục `MSSV` (vd, nếu bạn có MSSV là 1234567 thì bạn đặt tên thư mục là `1234567`) bạn đặt file: `BT06-Titanic.ipynb`, `train.csv`, `val.csv`, `test.csv`; rồi nén thư mục `MSSV` này lại và nộp ở link trên moodle.\n",
    "\n",
    "**Nội dung bài tập**\n",
    "\n",
    "Trong bài này, bạn sẽ thực hành: (i) tiền xử lý dữ liệu, và (ii) huấn luyện Neural Net với weight decay và early stopping. Bộ dữ liệu được sử dụng là bộ [Kaggle Titanic](https://www.kaggle.com/c/titanic); trong đó, đầu vào là thông tin của hành khách trên tàu Titanic (bạn xem chi tiết trong file `description.txt` đính kèm), đầu ra là một trong hai lớp sống/chết (1/0). Mình có đính kèm các file dữ liệu: `train.csv` - tập huấn luyện, `val.csv` - tập validation, `test.csv` - tập kiểm tra (chỉ có đầu vào); thật ra, Kaggle chỉ cung cấp hai file là `train.csv` và `test.csv`, mình đã tách file `train.csv` của Kaggle ra hai file là `train.csv` (80%) và `val.csv` (20%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.mode.chained_assignment = None\n",
    "# import cPickle\n",
    "# You can also import other things ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Đọc dữ liệu tập huấn luyện và tập validation vào data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 712 entries, 496 to 153\n",
      "Data columns (total 11 columns):\n",
      "Survived    712 non-null int64\n",
      "Pclass      712 non-null int64\n",
      "Name        712 non-null object\n",
      "Sex         712 non-null object\n",
      "Age         574 non-null float64\n",
      "SibSp       712 non-null int64\n",
      "Parch       712 non-null int64\n",
      "Ticket      712 non-null object\n",
      "Fare        712 non-null float64\n",
      "Cabin       160 non-null object\n",
      "Embarked    711 non-null object\n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 66.8+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Yousseff, Mr. Gerious</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2627</td>\n",
       "      <td>14.4583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Willey, Mr. Edward</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S.O./P.P. 751</td>\n",
       "      <td>7.5500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Rice, Master. Eric</td>\n",
       "      <td>male</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>382652</td>\n",
       "      <td>29.1250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Spencer, Mrs. William Augustus (Marie Eugenie)</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17569</td>\n",
       "      <td>146.5208</td>\n",
       "      <td>B78</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Touma, Mrs. Darwis (Hanne Youssef Razi)</td>\n",
       "      <td>female</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2650</td>\n",
       "      <td>15.2458</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Survived  Pclass                                            Name  \\\n",
       "PassengerId                                                                     \n",
       "496                 0       3                           Yousseff, Mr. Gerious   \n",
       "649                 0       3                              Willey, Mr. Edward   \n",
       "279                 0       3                              Rice, Master. Eric   \n",
       "32                  1       1  Spencer, Mrs. William Augustus (Marie Eugenie)   \n",
       "256                 1       3         Touma, Mrs. Darwis (Hanne Youssef Razi)   \n",
       "\n",
       "                Sex   Age  SibSp  Parch         Ticket      Fare Cabin  \\\n",
       "PassengerId                                                              \n",
       "496            male   NaN      0      0           2627   14.4583   NaN   \n",
       "649            male   NaN      0      0  S.O./P.P. 751    7.5500   NaN   \n",
       "279            male   7.0      4      1         382652   29.1250   NaN   \n",
       "32           female   NaN      1      0       PC 17569  146.5208   B78   \n",
       "256          female  29.0      0      2           2650   15.2458   NaN   \n",
       "\n",
       "            Embarked  \n",
       "PassengerId           \n",
       "496                C  \n",
       "649                S  \n",
       "279                Q  \n",
       "32                 C  \n",
       "256                C  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('train.csv', index_col=0)\n",
    "train_df.info()\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 179 entries, 529 to 685\n",
      "Data columns (total 11 columns):\n",
      "Survived    179 non-null int64\n",
      "Pclass      179 non-null int64\n",
      "Name        179 non-null object\n",
      "Sex         179 non-null object\n",
      "Age         140 non-null float64\n",
      "SibSp       179 non-null int64\n",
      "Parch       179 non-null int64\n",
      "Ticket      179 non-null object\n",
      "Fare        179 non-null float64\n",
      "Cabin       44 non-null object\n",
      "Embarked    178 non-null object\n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 16.8+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Salonen, Mr. Johan Werner</td>\n",
       "      <td>male</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3101296</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Kelly, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>363592</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Kimball, Mr. Edwin Nelson Jr</td>\n",
       "      <td>male</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11753</td>\n",
       "      <td>52.5542</td>\n",
       "      <td>D19</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Morley, Mr. William</td>\n",
       "      <td>male</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>364506</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Boulos, Mr. Hanna</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2664</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Survived  Pclass                          Name   Sex   Age  \\\n",
       "PassengerId                                                               \n",
       "529                 0       3     Salonen, Mr. Johan Werner  male  39.0   \n",
       "697                 0       3              Kelly, Mr. James  male  44.0   \n",
       "622                 1       1  Kimball, Mr. Edwin Nelson Jr  male  42.0   \n",
       "462                 0       3           Morley, Mr. William  male  34.0   \n",
       "599                 0       3             Boulos, Mr. Hanna  male   NaN   \n",
       "\n",
       "             SibSp  Parch   Ticket     Fare Cabin Embarked  \n",
       "PassengerId                                                 \n",
       "529              0      0  3101296   7.9250   NaN        S  \n",
       "697              0      0   363592   8.0500   NaN        S  \n",
       "622              1      0    11753  52.5542   D19        S  \n",
       "462              0      0   364506   8.0500   NaN        S  \n",
       "599              0      0     2664   7.2250   NaN        C  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df = pd.read_csv('val.csv', index_col=0)\n",
    "val_df.info()\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiền xử lý"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Tiền xử lý tập huấn luyện**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_input_df = train_df.iloc[:, 1:]\n",
    "train_output_df = train_df.Survived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1. Xây dựng `train_X` từ `train_input_df`**\n",
    "\n",
    "(`train_X` là mảng numpy chứa các véc-tơ đầu vào mà có thể đưa trực tiếp vào mô hình học như ở các bài tập trước.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đầu tiên, ta sẽ tiến hành: \n",
    "- Bỏ cột `Cabin` vì cột này có nhiều giá thiếu (552/712) (và có vẻ cột này sẽ không giúp ích được gì nhiều cho việc dự đoán sống/chết).\n",
    "- Bỏ cột `Ticket` vì cột này có giá trị không phải dạng số, sẽ cần phải tốn sức để chuyển sang dạng số (và có vẻ cột này cũng sẽ không giúp ích được gì nhiều cho việc dự đoán sống/chết).\n",
    "- Bỏ cột `Name` vì cột này có giá trị không phải dạng số, sẽ cần phải tốn sức để chuyển sang dạng số. Lưu ý là, việc bỏ cột `Name` có thể sẽ làm mất mát thông tin cần thiết để dự đoán sống/chết (vì trong cột `Name` có các từ như là `Miss`, `Mrs`, `Mr`, ... có thể sẽ có ích cho việc dự đoán); tuy nhiên, ở đây, để đơn giản, ta bỏ luôn :-)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 712 entries, 496 to 153\n",
      "Data columns (total 7 columns):\n",
      "Pclass      712 non-null int64\n",
      "Sex         712 non-null object\n",
      "Age         574 non-null float64\n",
      "SibSp       712 non-null int64\n",
      "Parch       712 non-null int64\n",
      "Fare        712 non-null float64\n",
      "Embarked    711 non-null object\n",
      "dtypes: float64(2), int64(3), object(2)\n",
      "memory usage: 44.5+ KB\n"
     ]
    }
   ],
   "source": [
    "dropped_cols = ['Name', 'Ticket', 'Cabin']\n",
    "train_input_df.drop(dropped_cols, axis=1, inplace=True)\n",
    "train_input_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kế đến, ta sẽ xử lý các giá trị thiếu ở cột `Age` và cột `Embarked`. Ở đây, ta xử lý đơn giản là điền giá trị mean (giá trị trung bình) vào các giá trị thiếu ở cột `Age`, và điền giá trị mode (giá trị xuất hiện nhiều nhất) vào các giá trị thiếu ở cột `Embarked`. *Lưu ý là, khi điền các giá trị thiếu ở tập validation/test, ta sẽ sử dụng các giá trị mean/mode được tính từ tập huấn luyện*. Trong thực tế,  ta thường sẽ không biết ở thời điểm test biến đầu vào nào sẽ thiếu giá trị; do đó, ta có thể làm một cách tổng quát như sau: với biến đầu vào có giá trị số, ta sẽ điền giá trị mean; ngược lại, điền giá trị mode. Dưới đây, hàm `compute_mean_mode` sẽ tính từ tập huấn luyện giá trị mean/mode của *tất cả* các biến đầu vào; hàm `fill_missing_values` sẽ dùng các giá trị mean/mode này để điền giá trị thiếu cho một tập dữ liệu nào đó (tập huấn luyện/validation/kiểm tra)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_mean_mode(train_input_df):\n",
    "    '''\n",
    "    Computes means for numeric input variables and modes for non-numeric ones.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    train_input_df : data frame\n",
    "        The data frame containing training inputs.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mean_mode_dict : dictionary, len = # input variables (# columns) of train_input_df\n",
    "        mean_mode_dict[<column_name>] = mean/mode of this column.\n",
    "    '''\n",
    "    \n",
    "    mean_mode_dict = dict()\n",
    "    \n",
    "    for label in train_input_df.columns:\n",
    "        label_dtype = train_input_df[label].dtype\n",
    "        \n",
    "        if label_dtype == \"int64\" or label_dtype == \"float64\":\n",
    "            mean_mode_dict[label] = train_input_df[label].mean()\n",
    "        else:\n",
    "            mean_mode_dict[label] = train_input_df[label].mode()[0]\n",
    "    \n",
    "    return mean_mode_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Age': 29.488815331010457,\n",
       " 'Embarked': 'S',\n",
       " 'Fare': 32.18301095505614,\n",
       " 'Parch': 0.37359550561797755,\n",
       " 'Pclass': 2.3230337078651684,\n",
       " 'Sex': 'male',\n",
       " 'SibSp': 0.5140449438202247}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_mode_dict = compute_mean_mode(train_input_df)\n",
    "mean_mode_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kết quả của mình:\n",
    "\n",
    "`{'Age': 29.488815331010457,\n",
    " 'Embarked': 'S',\n",
    " 'Fare': 32.18301095505614,\n",
    " 'Parch': 0.37359550561797755,\n",
    " 'Pclass': 2.3230337078651684,\n",
    " 'Sex': 'male',\n",
    " 'SibSp': 0.5140449438202247}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fill_missing_values(input_df, mean_mode_dict):\n",
    "    '''\n",
    "    Fills missing values for ALL columns of `input_df` using `mean_mode_dict`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_df : data frame\n",
    "        The data frame containing inputs.\n",
    "    mean_mode_dict : dictionary\n",
    "        mean_mode_dict[<column_name>] = mean/mode of this column (estimated from the training set).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    filled_input_df : data frame\n",
    "        The data frame containing inputs after filling missing values.\n",
    "    '''\n",
    "        \n",
    "    filled_input_df = input_df.copy()\n",
    "    \n",
    "    for label in filled_input_df.columns:\n",
    "        filled_input_df[label] = filled_input_df[label].replace(np.nan, mean_mode_dict[label])\n",
    "    \n",
    "    return filled_input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId\n",
      "496   NaN\n",
      "649   NaN\n",
      "32    NaN\n",
      "299   NaN\n",
      "368   NaN\n",
      "Name: Age, dtype: float64\n",
      "\n",
      "PassengerId\n",
      "62    NaN\n",
      "Name: Embarked, dtype: object\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 712 entries, 496 to 153\n",
      "Data columns (total 7 columns):\n",
      "Pclass      712 non-null int64\n",
      "Sex         712 non-null object\n",
      "Age         574 non-null float64\n",
      "SibSp       712 non-null int64\n",
      "Parch       712 non-null int64\n",
      "Fare        712 non-null float64\n",
      "Embarked    711 non-null object\n",
      "dtypes: float64(2), int64(3), object(2)\n",
      "memory usage: 44.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# Before filling\n",
    "missing_age_mask = train_input_df.Age.isnull()\n",
    "missing_embarked_mask = train_input_df.Embarked.isnull()\n",
    "print train_input_df.Age[missing_age_mask].head()\n",
    "print\n",
    "print train_input_df.Embarked[missing_embarked_mask].head()\n",
    "train_input_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId\n",
      "496    29.488815\n",
      "649    29.488815\n",
      "32     29.488815\n",
      "299    29.488815\n",
      "368    29.488815\n",
      "Name: Age, dtype: float64\n",
      "\n",
      "PassengerId\n",
      "62    S\n",
      "Name: Embarked, dtype: object\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 712 entries, 496 to 153\n",
      "Data columns (total 7 columns):\n",
      "Pclass      712 non-null int64\n",
      "Sex         712 non-null object\n",
      "Age         712 non-null float64\n",
      "SibSp       712 non-null int64\n",
      "Parch       712 non-null int64\n",
      "Fare        712 non-null float64\n",
      "Embarked    712 non-null object\n",
      "dtypes: float64(2), int64(3), object(2)\n",
      "memory usage: 44.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# Fill and check the result\n",
    "train_input_df = fill_missing_values(train_input_df, mean_mode_dict)\n",
    "print train_input_df.Age[missing_age_mask].head()\n",
    "print\n",
    "print train_input_df.Embarked[missing_embarked_mask].head()\n",
    "print\n",
    "train_input_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đến đây, ta đã bỏ các cột `Name`, `Ticket`, `Cabin`; và điền giá trị thiếu cho cột `Age`, `Embarked`. Kế đến, ta sẽ chuyển các cột có giá trị không phải dạng số (`Sex` và `Embarked`) sang dạng số. Vì `Sex` và `Embarked` là các biến có giá trị rời-rạc và không-có-thứ-tự nên ta có thể chuyển sang dạng \"one-hot\" (và bỏ cột cuối). Vd:\n",
    "\n",
    "```\n",
    "Sex    --> Female | Male\n",
    "------     --------------\n",
    "female --> 1      | 0\n",
    "male   --> 0      | 1\n",
    "female --> 1      | 0\n",
    "```\n",
    "và ta có thể bỏ cột `Male` đi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_categorical_cols(input_df):\n",
    "    '''\n",
    "    Converts `Sex` column and `Embarked` column to one-hot forms.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_df : data frame\n",
    "        The data frame containing inputs.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    numeric_input_df : data frame\n",
    "        The data frame containing inputs after converting.\n",
    "    '''\n",
    "    \n",
    "    numeric_input_df = input_df.copy()\n",
    "    \n",
    "    len_df = len(numeric_input_df['Age'])\n",
    "    \n",
    "    numeric_input_df['Female'] = pd.Series(float(0), index=numeric_input_df.index)\n",
    "    numeric_input_df['C'] = pd.Series(float(0), index=numeric_input_df.index)\n",
    "    numeric_input_df['Q'] = pd.Series(float(0), index=numeric_input_df.index)\n",
    "    \n",
    "    numeric_input_df.Female[numeric_input_df.Sex == 'female'] = 1\n",
    "    numeric_input_df.C[numeric_input_df.Embarked == 'C'] = 1\n",
    "    numeric_input_df.Q[numeric_input_df.Embarked == 'Q'] = 1\n",
    "    \n",
    "    numeric_input_df.drop('Sex', axis=1, inplace=True)\n",
    "    numeric_input_df.drop('Embarked', axis=1, inplace=True)\n",
    "    \n",
    "    return numeric_input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 712 entries, 496 to 153\n",
      "Data columns (total 8 columns):\n",
      "Pclass    712 non-null int64\n",
      "Age       712 non-null float64\n",
      "SibSp     712 non-null int64\n",
      "Parch     712 non-null int64\n",
      "Fare      712 non-null float64\n",
      "Female    712 non-null float64\n",
      "C         712 non-null float64\n",
      "Q         712 non-null float64\n",
      "dtypes: float64(5), int64(3)\n",
      "memory usage: 50.1 KB\n"
     ]
    }
   ],
   "source": [
    "train_input_df = process_categorical_cols(train_input_df)\n",
    "train_input_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kết quả của mình:\n",
    "\n",
    "`<class 'pandas.core.frame.DataFrame'>\n",
    "Int64Index: 712 entries, 496 to 153\n",
    "Data columns (total 8 columns):\n",
    "Pclass    712 non-null int64\n",
    "Age       712 non-null float64\n",
    "SibSp     712 non-null int64\n",
    "Parch     712 non-null int64\n",
    "Fare      712 non-null float64\n",
    "female    712 non-null float64\n",
    "C         712 non-null float64\n",
    "Q         712 non-null float64\n",
    "dtypes: float64(5), int64(3)\n",
    "memory usage: 50.1 KB`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuối cùng, ta sẽ xây dựng `train_X`. Để giúp Gradient Descent hội tụ nhanh hơn, ta sẽ chuẩn hóa để các cột của `train_X` có mean bằng 0 và có độ lệch chuẩn bằng 1. *Lưu ý là, khi chuẩn hóa `val_X`/`test_X`, ta sẽ dùng mean và độ lệch chuẩn được ước lượng từ tập huấn luyện.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(712L, 8L)\n"
     ]
    }
   ],
   "source": [
    "train_X = train_input_df.values\n",
    "print train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_mean = train_X.mean(axis=0)\n",
    "X_std = train_X.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2.32303371  29.48881533   0.51404494   0.37359551  32.18301096\n",
      "   0.3497191    0.17977528   0.08426966]\n",
      "[  0.83439261  13.12054565   1.0743822    0.80082787  52.29476578\n",
      "   0.47688117   0.38400017   0.2777918 ]\n"
     ]
    }
   ],
   "source": [
    "print X_mean\n",
    "print X_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kết quả của mình:\n",
    "\n",
    "`[  2.32303371  29.48881533   0.51404494   0.37359551  32.18301096\n",
    "   0.3497191    0.17977528   0.08426966]\n",
    "[  0.83439261  13.12054565   1.0743822    0.80082787  52.29476578\n",
    "   0.47688117   0.38400017   0.2777918 ]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  8.11328249e-01   2.70774842e-16  -4.78456309e-01 ...,  -7.33346423e-01\n",
      "    2.13600094e+00  -3.03355468e-01]\n",
      " [  8.11328249e-01   2.70774842e-16  -4.78456309e-01 ...,  -7.33346423e-01\n",
      "   -4.68164589e-01  -3.03355468e-01]\n",
      " [  8.11328249e-01  -1.71401525e+00   3.24461355e+00 ...,  -7.33346423e-01\n",
      "   -4.68164589e-01   3.29646275e+00]\n",
      " ..., \n",
      " [ -1.58562492e+00   1.33463845e+00  -4.78456309e-01 ...,  -7.33346423e-01\n",
      "   -4.68164589e-01  -3.03355468e-01]\n",
      " [ -3.87148335e-01   9.53556735e-01  -4.78456309e-01 ...,  -7.33346423e-01\n",
      "   -4.68164589e-01  -3.03355468e-01]\n",
      " [  8.11328249e-01   1.98247736e+00  -4.78456309e-01 ...,  -7.33346423e-01\n",
      "   -4.68164589e-01  -3.03355468e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Normalize train_X using X_mean and X_std\n",
    "train_X = (train_X - X_mean) / X_std\n",
    "print train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2.09570189e-16  -4.98976640e-18   1.24744160e-18  -4.24130144e-17\n",
      "  -5.48874304e-17   6.48669632e-17   2.49488320e-17   7.48464960e-18]\n",
      "[ 1.  1.  1.  1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "print train_X.mean(axis=0)\n",
    "print train_X.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(712L, 9L)\n"
     ]
    }
   ],
   "source": [
    "# Add x_0 column\n",
    "train_X = np.hstack([np.ones((len(train_X), 1)), train_X])\n",
    "print train_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**1.2. Xây dựng `train_Y` từ `train_output_df`**\n",
    "\n",
    "(`train_Y` là mảng numpy chứa các đầu ra đúng mà có thể đưa trực tiếp vào mô hình học như ở các bài tập trước.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(712L, 1L)\n"
     ]
    }
   ],
   "source": [
    "train_Y = train_output_df.values.reshape(-1, 1)\n",
    "print train_Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Tiền xử lý tập validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_input_df = val_df.iloc[:, 1:]\n",
    "val_output_df = val_df.Survived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1. Xây dựng `val_X` từ `val_input_df`**\n",
    "\n",
    "Cách xây dựng `test_X` từ `test_input_df` sẽ *giống hệt* cách xây dựng `val_X` từ `val_input_df`. Do đó, ta sẽ viết phần xử lý này vào một hàm để lúc sau có thể dùng lại cho tập kiểm tra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_new_input_df(new_input_df, dropped_cols, mean_mode_dict, X_mean, X_std):\n",
    "    '''\n",
    "    Builds `new_X` from `new_input_df`:\n",
    "    1. Drop columns using `dropped_cols` (`dropped_cols` is the list containing names of dropped columns)\n",
    "    2. Fill missing values using `mean_mode_dict` (use `fill_missing_values` function)\n",
    "    3. Convert categorial columns to one-hot (use `process_categorical_cols` function)\n",
    "    4. Subtract by `X_mean` and divide by `X_std`\n",
    "    5. Add `x_0` column\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    I'm lazy now ...\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    new_X : numpy array\n",
    "        The matrix of input vectors.\n",
    "    '''\n",
    "    \n",
    "    dropped_cols = ['Name', 'Ticket', 'Cabin']\n",
    "    new_input_df.drop(dropped_cols, axis=1, inplace=True)\n",
    "    \n",
    "    new_input_df = fill_missing_values(new_input_df, mean_mode_dict)\n",
    "    \n",
    "    new_input_df = process_categorical_cols(new_input_df)\n",
    "    \n",
    "    new_X = new_input_df.values\n",
    "    \n",
    "    new_X = (new_X - X_mean) / X_std\n",
    "    \n",
    "    new_X = np.hstack([np.ones((len(new_X), 1)), new_X])\n",
    "    \n",
    "    return new_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179L, 9L)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_X = process_new_input_df(val_input_df, dropped_cols, mean_mode_dict, X_mean, X_std)\n",
    "val_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2. Xây dựng `val_Y` từ `val_output_df`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179L, 1L)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_Y = val_df.Survived.values.reshape(-1, 1)\n",
    "val_Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huấn luyện"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hàm tính output của Neural Net** (giống hệt bài tập trước)\n",
    "\n",
    "Ở đây, ta dùng hàm kích hoạt sigmoid ở các tầng ẩn, và hàm softmax ở tầng cuối."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(S):\n",
    "    '''\n",
    "    Computes sigmoid function for each element of array S.\n",
    "    '''\n",
    "    return 1 / (1 + np.exp(-S))\n",
    "def softmax(S):\n",
    "    '''\n",
    "    Computes softmax function for each row of array S.\n",
    "    '''\n",
    "    A = np.exp(S)\n",
    "    A /= A.sum(axis=1, keepdims=True)\n",
    "    return A\n",
    "def compute_nnet_outputs(Ws, X, need_all_layer_outputs):\n",
    "    '''\n",
    "    Computes the outputs of Neural Net by forward propagating X through the net.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Ws : list of numpy arrays\n",
    "        Ws[l-1] is W of layer l with l >= 1 (layer 0 is input layer; it doesn't have W);\n",
    "        W of layer l will have the shape of (d^(l-1)+1, d^(l)), where \n",
    "        d^(l-1) is the number of neurons (not count the +1 neuron) of layer l-1 and \n",
    "        d^(l) is the number of neurons (not count the +1 neuron) of layer l.\n",
    "    X : numpy array, shape (N, d+1)\n",
    "        The matrix of input vectors (each row corresponds to an input vector); \n",
    "        the first column of this matrix is all ones (corresponding to x_0).\n",
    "    need_all_layer_outputs : bool\n",
    "        If this var is true, we'll return a list of layer's-outputs; \n",
    "        otherwise, we'll return the final layer's output.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    If `need_all_layer_outputs` is false, return\n",
    "        A : numpy array, shape (N, K=10)\n",
    "            The maxtrix of output vectors of final layer; each row is an output vector (containing \n",
    "            each class's probability given the corresponding input vector).\n",
    "    Else, return\n",
    "        As : list of numpy arrays\n",
    "            As[l] is the matrix of output vectors of layer l; each row is an output vector (corresponding \n",
    "            to an input vector).\n",
    "    '''    \n",
    "    \n",
    "    As = []\n",
    "    As.append(X)\n",
    "    \n",
    "    for i in xrange(len(Ws)):\n",
    "        Z = np.dot(As[i], Ws[i])\n",
    "\n",
    "        if (i == len(Ws)-1):\n",
    "            A = softmax(Z)\n",
    "            As.append(A)\n",
    "        else:\n",
    "            temp = sigmoid(Z)\n",
    "            temp = np.hstack([np.ones((len(temp), 1)), temp])\n",
    "            As.append(temp)\n",
    "        \n",
    "    if need_all_layer_outputs:\n",
    "        return As\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_weight_decay(Ws):\n",
    "    result = 0\n",
    "    for i in xrange(len(Ws)):\n",
    "        result += np.sum(Ws[i]**2)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hàm huấn luyện Neural Net** (bổ sung weight decay và early stopping so với bài tập trước)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_nnet(train_X, train_Y, val_X, val_Y, hid_layer_sizes, wd_level,\n",
    "               mb_size, learning_rate, max_patience, max_epoch=1000000, momentum_param=0.):\n",
    "    '''\n",
    "    Trains Neural Net on the dataset (X, Y).\n",
    "    Cost function: mean negative log likelihood + weight decay.\n",
    "    Optimization algorithm: SGD; stopping criteria: early stopping and/or max epoch.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array, shape (N, d + 1)\n",
    "        The matrix of input vectors (each row corresponds to an input vector);\n",
    "        the first column of this matrix is all ones (corresponding to x_0).\n",
    "    Y : numpy array, shape (N, 1)\n",
    "        The vector of outputs.\n",
    "    hid_layer_sizes : list\n",
    "        The list of hidden layer sizes; e.g., hid_layer_sizes = [20, 10] means:\n",
    "        the Net has 2 hidden layers, the 1st one has 20 neurons, and the 2nd one has \n",
    "        10 neurons (not count the +1 neurons).\n",
    "    wd_level : float\n",
    "        The level (coefficient) of weight decay.\n",
    "    mb_size : int\n",
    "        Minibatch size of SGD.\n",
    "    learning_rate : float\n",
    "        Learning rate of SGD.\n",
    "    max_patience : int (> 0) or None\n",
    "        The parameter of early stopping. We'll have a `patience` variable with initial value equal to\n",
    "        `max_patience`. During the training, we'll keep track of the best MBE (Mean Binary Error) \n",
    "        on the validation set; if the MBE on the validation set at the current epoch < the current \n",
    "        best one, we'll reset `patience` to `max_patience`; otherwise, `patience` -= 1.\n",
    "        When `patience` = 0 or `max_epoch` is reached, we'll terminate SGD.\n",
    "        If `max_patience` is None, we don't use early stopping.\n",
    "    max_epoch : int, default 1000000\n",
    "        We'll terminate SGD after this number of epochs or when `patience` = 0 (if early stopping is used).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Ws : list of numpy arrays\n",
    "        Ws[l-1] is W of layer l with l >= 1 (layer 0 is input layer; it doesn't have W);\n",
    "        W of layer l will have the shape of (d^(l-1)+1, d^(l)), where \n",
    "        d^(l-1) is the number of neurons (not count the +1 neuron) of layer l-1 and \n",
    "        d^(l) is the number of neurons (not count the +1 neuron) of layer l.\n",
    "        *If `max_patience` is None, Ws are weights after the final epoch (as previous homeworks); \n",
    "        otherwise, Ws are weights corresponding to the best MBE on the validation set.*\n",
    "    train_errs : list, len = num epochs spent on training\n",
    "        The list of MBEs on the training set after each epoch.\n",
    "    val_errs : list, len = num epochs spent on training\n",
    "        The list of MBEs on the validation set after each epoch.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    After each *100-epochs* (in the experiments below, you'll not want to print after each single epoch), \n",
    "    you need to print out: \n",
    "    - The MBE on the training set and validation set (regardless of `max_patience`).\n",
    "    - The value of `patience` (if `max_patience` is not None).\n",
    "    E.g., `Epoch ..., training err ..., val err ..., patience ...` (in this case, `max_patience` is not None).\n",
    "    \n",
    "    After the training, you need to print out the info of returned Ws:\n",
    "    - The corresponding epoch.\n",
    "    - The corresponding MBE on the training set and validation set.\n",
    "    E.g., `Info of returned Ws: epoch ..., train err ..., val err ...`.\n",
    "    '''\n",
    "    # Prepare for training\n",
    "    K = len(np.unique(train_Y)) # Num classes\n",
    "    layer_sizes = [train_X.shape[1] - 1] + hid_layer_sizes + [K]\n",
    "    np.random.seed(0) # This will fix the randomization; so, you and me will have the same results\n",
    "    Ws = [np.random.randn(layer_sizes[i] + 1, layer_sizes[i + 1]) / np.sqrt(layer_sizes[i] + 1)\n",
    "          for i in range(len(layer_sizes) - 1)] # Init Ws\n",
    "    \n",
    "    one_hot_Y = np.zeros((len(train_Y), K))\n",
    "    one_hot_Y[np.arange(len(train_Y)), train_Y.reshape(-1)] = 1\n",
    "    train_costs = [] # To save costs during training\n",
    "    train_errs = [] # To save mean binary errors during training\n",
    "    \n",
    "    one_hot_val_Y = np.zeros((len(val_Y), K))\n",
    "    one_hot_val_Y[np.arange(len(val_Y)), val_Y.reshape(-1)] = 1\n",
    "    val_costs = [] # To save costs during validiting\n",
    "    val_errs = [] # To save mean binary errors during validating\n",
    "    \n",
    "    N = len(train_X) # Num training examples\n",
    "    rnd_idxs = range(N) # Random indexes\n",
    "\n",
    "    patience = max_patience\n",
    "    best_MBE = 0\n",
    "    # Train\n",
    "    for epoch in range(max_epoch):\n",
    "        \n",
    "        if max_patience != None and patience == 0:\n",
    "            break\n",
    "        \n",
    "        np.random.shuffle(rnd_idxs)\n",
    "        for start_idx in range(0, N, mb_size):\n",
    "            # Get minibach\n",
    "            mb_X = train_X[rnd_idxs[start_idx:start_idx+mb_size]]\n",
    "            mb_Y = one_hot_Y[rnd_idxs[start_idx:start_idx+mb_size]]\n",
    "            \n",
    "            # Forward-prop\n",
    "            As = compute_nnet_outputs(Ws, mb_X, True)\n",
    "            \n",
    "            # Back-prop; on the way, compute each layer's gradient and update its W\n",
    "            delta = As[-1] - mb_Y\n",
    "            grad = np.dot(As[-2].T, delta)/len(mb_X)\n",
    "            Ws[-1] -= learning_rate * (grad + 2 * wd_level * Ws[-1])\n",
    "            for i in range(2, len(Ws) + 1):\n",
    "                delta = delta.dot(Ws[-i + 1].T[:, 1:]) * As[-i][:, 1:] * (1 - As[-i][:, 1:])\n",
    "                grad = np.dot(As[-i-1].T, delta)/len(mb_X)\n",
    "                Ws[-i] -= learning_rate * (grad + 2 * wd_level * Ws[-i])\n",
    "        \n",
    "        # Compute training info, save it, and print it\n",
    "        A = compute_nnet_outputs(Ws, train_X, False)\n",
    "        train_cost = np.mean(-np.sum(one_hot_Y * np.log(A), axis=1)) + wd_level  * compute_weight_decay(Ws)\n",
    "        train_err = np.mean(np.argmax(A, axis=1) != train_Y.squeeze()) * 100\n",
    "        train_costs.append(train_cost)\n",
    "        train_errs.append(train_err)\n",
    "        \n",
    "        # Compute validating info, save it, and print it\n",
    "        val_A = compute_nnet_outputs(Ws, val_X, False)\n",
    "        val_cost = np.mean(-np.sum(one_hot_val_Y * np.log(val_A), axis=1)) + wd_level * compute_weight_decay(Ws)\n",
    "        val_err = np.mean(np.argmax(val_A, axis=1) != val_Y.squeeze()) * 100\n",
    "        val_costs.append(val_cost)\n",
    "        val_errs.append(val_err)\n",
    "        \n",
    "        if max_patience != None:\n",
    "            if epoch == 0:\n",
    "                best_MBE = val_err\n",
    "            else:\n",
    "                if val_err < best_MBE:\n",
    "                    patience = max_patience\n",
    "                else:\n",
    "                    patience -= 1\n",
    "        \n",
    "        if max_patience != None and epoch % 100 == 0:\n",
    "            print \"Epoch %d, train err %.3f, val err %.3f, patience %d\" % (epoch, train_err, val_err, patience)\n",
    "        \n",
    "        print \"Info of returned Ws: epoch %d, train err %.3f, val err %.3f\" % (epoch, train_err, val_err)\n",
    "\n",
    "    return Ws, train_errs, val_errs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thí nghiệm 1: ảnh hưởng của weight decay**\n",
    "\n",
    "Để thấy được ảnh hưởng của weight decay tới quá trình học, ta sẽ huấn luyện Neural Net với các giá trị `wd_level` khác nhau: 0., 0.001, 0.005; ở đây, ta cố định `hid_layer_sizes=[50], mb_size=len(train_X), learning_rate=0.1, max_patience=None, max_epoch=8000`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info of returned Ws: epoch 0, train err 34.972, val err 41.899\n",
      "Info of returned Ws: epoch 1, train err 36.376, val err 45.251\n",
      "Info of returned Ws: epoch 2, train err 36.376, val err 45.251\n",
      "Info of returned Ws: epoch 3, train err 36.376, val err 44.693\n",
      "Info of returned Ws: epoch 4, train err 35.815, val err 44.134\n",
      "Info of returned Ws: epoch 5, train err 35.674, val err 44.134\n",
      "Info of returned Ws: epoch 6, train err 35.534, val err 43.575\n",
      "Info of returned Ws: epoch 7, train err 35.112, val err 43.017\n",
      "Info of returned Ws: epoch 8, train err 34.551, val err 41.899\n",
      "Info of returned Ws: epoch 9, train err 33.848, val err 41.899\n",
      "Info of returned Ws: epoch 10, train err 33.146, val err 41.341\n",
      "Info of returned Ws: epoch 11, train err 33.146, val err 41.341\n",
      "Info of returned Ws: epoch 12, train err 32.584, val err 41.341\n",
      "Info of returned Ws: epoch 13, train err 31.461, val err 40.223\n",
      "Info of returned Ws: epoch 14, train err 30.899, val err 39.665\n",
      "Info of returned Ws: epoch 15, train err 30.478, val err 39.106\n",
      "Info of returned Ws: epoch 16, train err 30.197, val err 37.989\n",
      "Info of returned Ws: epoch 17, train err 29.775, val err 36.313\n",
      "Info of returned Ws: epoch 18, train err 28.511, val err 35.754\n",
      "Info of returned Ws: epoch 19, train err 27.528, val err 34.637\n",
      "Info of returned Ws: epoch 20, train err 26.966, val err 34.637\n",
      "Info of returned Ws: epoch 21, train err 27.107, val err 34.637\n",
      "Info of returned Ws: epoch 22, train err 27.107, val err 33.520\n",
      "Info of returned Ws: epoch 23, train err 26.966, val err 33.520\n",
      "Info of returned Ws: epoch 24, train err 26.685, val err 34.078\n",
      "Info of returned Ws: epoch 25, train err 26.826, val err 33.520\n",
      "Info of returned Ws: epoch 26, train err 26.404, val err 34.078\n",
      "Info of returned Ws: epoch 27, train err 26.404, val err 32.961\n",
      "Info of returned Ws: epoch 28, train err 25.702, val err 30.726\n",
      "Info of returned Ws: epoch 29, train err 24.860, val err 30.726\n",
      "Info of returned Ws: epoch 30, train err 24.719, val err 30.168\n",
      "Info of returned Ws: epoch 31, train err 23.876, val err 30.726\n",
      "Info of returned Ws: epoch 32, train err 23.876, val err 30.168\n",
      "Info of returned Ws: epoch 33, train err 23.315, val err 30.168\n",
      "Info of returned Ws: epoch 34, train err 22.893, val err 30.168\n",
      "Info of returned Ws: epoch 35, train err 22.893, val err 29.609\n",
      "Info of returned Ws: epoch 36, train err 22.472, val err 29.050\n",
      "Info of returned Ws: epoch 37, train err 22.051, val err 29.050\n",
      "Info of returned Ws: epoch 38, train err 21.629, val err 28.492\n",
      "Info of returned Ws: epoch 39, train err 21.489, val err 27.374\n",
      "Info of returned Ws: epoch 40, train err 21.208, val err 27.374\n",
      "Info of returned Ws: epoch 41, train err 20.787, val err 26.816\n",
      "Info of returned Ws: epoch 42, train err 21.067, val err 26.257\n",
      "Info of returned Ws: epoch 43, train err 21.629, val err 26.257\n",
      "Info of returned Ws: epoch 44, train err 21.348, val err 26.257\n",
      "Info of returned Ws: epoch 45, train err 21.770, val err 26.257\n",
      "Info of returned Ws: epoch 46, train err 21.489, val err 25.698\n",
      "Info of returned Ws: epoch 47, train err 21.489, val err 25.698\n",
      "Info of returned Ws: epoch 48, train err 21.629, val err 26.257\n",
      "Info of returned Ws: epoch 49, train err 21.629, val err 25.698\n",
      "Info of returned Ws: epoch 50, train err 21.489, val err 25.698\n",
      "Info of returned Ws: epoch 51, train err 21.489, val err 25.140\n",
      "Info of returned Ws: epoch 52, train err 21.489, val err 25.140\n",
      "Info of returned Ws: epoch 53, train err 21.489, val err 25.140\n",
      "Info of returned Ws: epoch 54, train err 21.489, val err 25.140\n",
      "Info of returned Ws: epoch 55, train err 21.489, val err 25.140\n",
      "Info of returned Ws: epoch 56, train err 21.067, val err 25.140\n",
      "Info of returned Ws: epoch 57, train err 21.208, val err 25.140\n",
      "Info of returned Ws: epoch 58, train err 21.208, val err 25.140\n",
      "Info of returned Ws: epoch 59, train err 21.067, val err 25.140\n",
      "Info of returned Ws: epoch 60, train err 20.927, val err 25.140\n",
      "Info of returned Ws: epoch 61, train err 21.208, val err 24.581\n",
      "Info of returned Ws: epoch 62, train err 21.348, val err 24.581\n",
      "Info of returned Ws: epoch 63, train err 21.348, val err 24.581\n",
      "Info of returned Ws: epoch 64, train err 21.348, val err 24.581\n",
      "Info of returned Ws: epoch 65, train err 21.067, val err 24.022\n",
      "Info of returned Ws: epoch 66, train err 20.787, val err 24.022\n",
      "Info of returned Ws: epoch 67, train err 20.787, val err 24.022\n",
      "Info of returned Ws: epoch 68, train err 20.927, val err 24.022\n",
      "Info of returned Ws: epoch 69, train err 21.067, val err 24.022\n",
      "Info of returned Ws: epoch 70, train err 21.067, val err 23.464\n",
      "Info of returned Ws: epoch 71, train err 21.208, val err 23.464\n",
      "Info of returned Ws: epoch 72, train err 21.067, val err 24.022\n",
      "Info of returned Ws: epoch 73, train err 20.787, val err 24.022\n",
      "Info of returned Ws: epoch 74, train err 20.927, val err 24.581\n",
      "Info of returned Ws: epoch 75, train err 21.208, val err 24.581\n",
      "Info of returned Ws: epoch 76, train err 21.067, val err 25.140\n",
      "Info of returned Ws: epoch 77, train err 21.208, val err 25.140\n",
      "Info of returned Ws: epoch 78, train err 21.208, val err 24.022\n",
      "Info of returned Ws: epoch 79, train err 21.208, val err 24.022\n",
      "Info of returned Ws: epoch 80, train err 21.208, val err 24.022\n",
      "Info of returned Ws: epoch 81, train err 21.067, val err 24.022\n",
      "Info of returned Ws: epoch 82, train err 21.067, val err 24.022\n",
      "Info of returned Ws: epoch 83, train err 20.787, val err 24.022\n",
      "Info of returned Ws: epoch 84, train err 20.927, val err 24.022\n",
      "Info of returned Ws: epoch 85, train err 20.927, val err 24.022\n",
      "Info of returned Ws: epoch 86, train err 20.646, val err 23.464\n",
      "Info of returned Ws: epoch 87, train err 20.787, val err 23.464\n",
      "Info of returned Ws: epoch 88, train err 20.927, val err 23.464\n",
      "Info of returned Ws: epoch 89, train err 20.646, val err 24.022\n",
      "Info of returned Ws: epoch 90, train err 20.646, val err 24.022\n",
      "Info of returned Ws: epoch 91, train err 20.787, val err 24.022\n",
      "Info of returned Ws: epoch 92, train err 20.927, val err 24.022\n",
      "Info of returned Ws: epoch 93, train err 21.067, val err 24.022\n",
      "Info of returned Ws: epoch 94, train err 21.067, val err 24.022\n",
      "Info of returned Ws: epoch 95, train err 21.067, val err 24.022\n",
      "Info of returned Ws: epoch 96, train err 21.208, val err 24.022\n",
      "Info of returned Ws: epoch 97, train err 21.348, val err 24.022\n",
      "Info of returned Ws: epoch 98, train err 21.348, val err 24.022\n",
      "Info of returned Ws: epoch 99, train err 21.348, val err 24.022\n",
      "Info of returned Ws: epoch 100, train err 21.208, val err 24.022\n",
      "Info of returned Ws: epoch 101, train err 21.208, val err 23.464\n",
      "Info of returned Ws: epoch 102, train err 21.208, val err 23.464\n",
      "Info of returned Ws: epoch 103, train err 21.067, val err 24.022\n",
      "Info of returned Ws: epoch 104, train err 21.067, val err 24.022\n",
      "Info of returned Ws: epoch 105, train err 21.208, val err 24.022\n",
      "Info of returned Ws: epoch 106, train err 21.208, val err 24.022\n",
      "Info of returned Ws: epoch 107, train err 21.208, val err 24.022\n",
      "Info of returned Ws: epoch 108, train err 21.067, val err 24.022\n",
      "Info of returned Ws: epoch 109, train err 21.067, val err 24.022\n",
      "Info of returned Ws: epoch 110, train err 21.067, val err 24.022\n",
      "Info of returned Ws: epoch 111, train err 21.067, val err 24.022\n",
      "Info of returned Ws: epoch 112, train err 21.067, val err 24.022\n",
      "Info of returned Ws: epoch 113, train err 21.208, val err 24.022\n",
      "Info of returned Ws: epoch 114, train err 21.208, val err 24.022\n",
      "Info of returned Ws: epoch 115, train err 21.208, val err 24.022\n",
      "Info of returned Ws: epoch 116, train err 21.208, val err 24.022\n",
      "Info of returned Ws: epoch 117, train err 21.208, val err 24.022\n",
      "Info of returned Ws: epoch 118, train err 21.208, val err 24.022\n",
      "Info of returned Ws: epoch 119, train err 20.787, val err 24.022\n",
      "Info of returned Ws: epoch 120, train err 20.787, val err 24.022\n",
      "Info of returned Ws: epoch 121, train err 20.084, val err 20.112\n",
      "Info of returned Ws: epoch 122, train err 20.084, val err 20.112\n",
      "Info of returned Ws: epoch 123, train err 20.225, val err 20.112\n",
      "Info of returned Ws: epoch 124, train err 20.225, val err 19.553\n",
      "Info of returned Ws: epoch 125, train err 20.225, val err 19.553\n",
      "Info of returned Ws: epoch 126, train err 20.365, val err 19.553\n",
      "Info of returned Ws: epoch 127, train err 20.506, val err 19.553\n",
      "Info of returned Ws: epoch 128, train err 20.506, val err 19.553\n",
      "Info of returned Ws: epoch 129, train err 20.506, val err 19.553\n",
      "Info of returned Ws: epoch 130, train err 20.365, val err 19.553\n",
      "Info of returned Ws: epoch 131, train err 20.365, val err 19.553\n",
      "Info of returned Ws: epoch 132, train err 20.365, val err 19.553\n",
      "Info of returned Ws: epoch 133, train err 20.225, val err 19.553\n",
      "Info of returned Ws: epoch 134, train err 20.225, val err 19.553\n",
      "Info of returned Ws: epoch 135, train err 20.225, val err 19.553\n",
      "Info of returned Ws: epoch 136, train err 20.225, val err 19.553\n",
      "Info of returned Ws: epoch 137, train err 20.225, val err 19.553\n",
      "Info of returned Ws: epoch 138, train err 20.225, val err 19.553\n",
      "Info of returned Ws: epoch 139, train err 20.225, val err 19.553\n",
      "Info of returned Ws: epoch 140, train err 20.225, val err 19.553\n",
      "Info of returned Ws: epoch 141, train err 20.225, val err 19.553\n",
      "Info of returned Ws: epoch 142, train err 20.225, val err 19.553\n",
      "Info of returned Ws: epoch 143, train err 20.365, val err 19.553\n",
      "Info of returned Ws: epoch 144, train err 20.365, val err 19.553\n",
      "Info of returned Ws: epoch 145, train err 20.365, val err 19.553\n",
      "Info of returned Ws: epoch 146, train err 20.365, val err 19.553\n",
      "Info of returned Ws: epoch 147, train err 20.365, val err 19.553\n",
      "Info of returned Ws: epoch 148, train err 20.365, val err 19.553\n",
      "Info of returned Ws: epoch 149, train err 20.365, val err 19.553\n",
      "Info of returned Ws: epoch 150, train err 20.365, val err 19.553\n",
      "Info of returned Ws: epoch 151, train err 20.365, val err 19.553\n",
      "Info of returned Ws: epoch 152, train err 20.365, val err 18.994\n",
      "Info of returned Ws: epoch 153, train err 20.365, val err 18.994\n",
      "Info of returned Ws: epoch 154, train err 20.365, val err 18.994\n",
      "Info of returned Ws: epoch 155, train err 20.365, val err 18.994\n",
      "Info of returned Ws: epoch 156, train err 20.365, val err 18.994\n",
      "Info of returned Ws: epoch 157, train err 20.365, val err 18.994\n",
      "Info of returned Ws: epoch 158, train err 20.365, val err 18.994\n",
      "Info of returned Ws: epoch 159, train err 20.225, val err 18.994\n",
      "Info of returned Ws: epoch 160, train err 20.225, val err 18.994\n",
      "Info of returned Ws: epoch 161, train err 20.225, val err 18.994\n",
      "Info of returned Ws: epoch 162, train err 20.365, val err 18.994\n",
      "Info of returned Ws: epoch 163, train err 20.365, val err 18.994\n",
      "Info of returned Ws: epoch 164, train err 20.365, val err 18.994\n",
      "Info of returned Ws: epoch 165, train err 20.365, val err 18.994\n",
      "Info of returned Ws: epoch 166, train err 20.365, val err 18.994\n",
      "Info of returned Ws: epoch 167, train err 20.365, val err 18.994\n",
      "Info of returned Ws: epoch 168, train err 20.365, val err 18.994\n",
      "Info of returned Ws: epoch 169, train err 20.365, val err 18.994\n",
      "Info of returned Ws: epoch 170, train err 20.365, val err 18.994\n",
      "Info of returned Ws: epoch 171, train err 20.365, val err 18.994\n",
      "Info of returned Ws: epoch 172, train err 20.365, val err 18.994\n",
      "Info of returned Ws: epoch 173, train err 20.365, val err 18.994\n",
      "Info of returned Ws: epoch 174, train err 20.365, val err 18.994\n",
      "Info of returned Ws: epoch 175, train err 20.365, val err 18.994\n",
      "Info of returned Ws: epoch 176, train err 20.365, val err 18.994\n",
      "Info of returned Ws: epoch 177, train err 20.365, val err 18.994\n",
      "Info of returned Ws: epoch 178, train err 20.365, val err 18.994\n",
      "Info of returned Ws: epoch 179, train err 20.365, val err 18.436\n",
      "Info of returned Ws: epoch 180, train err 20.365, val err 18.436\n",
      "Info of returned Ws: epoch 181, train err 20.365, val err 18.436\n",
      "Info of returned Ws: epoch 182, train err 20.225, val err 18.436\n",
      "Info of returned Ws: epoch 183, train err 20.225, val err 18.436\n",
      "Info of returned Ws: epoch 184, train err 20.365, val err 18.436\n",
      "Info of returned Ws: epoch 185, train err 20.365, val err 18.436\n",
      "Info of returned Ws: epoch 186, train err 20.365, val err 18.436\n",
      "Info of returned Ws: epoch 187, train err 20.225, val err 18.436\n",
      "Info of returned Ws: epoch 188, train err 20.225, val err 18.436\n",
      "Info of returned Ws: epoch 189, train err 20.084, val err 18.436\n",
      "Info of returned Ws: epoch 190, train err 20.084, val err 18.436\n",
      "Info of returned Ws: epoch 191, train err 20.084, val err 18.436\n",
      "Info of returned Ws: epoch 192, train err 20.084, val err 18.436\n",
      "Info of returned Ws: epoch 193, train err 20.084, val err 18.436\n",
      "Info of returned Ws: epoch 194, train err 20.084, val err 18.436\n",
      "Info of returned Ws: epoch 195, train err 20.084, val err 18.436\n",
      "Info of returned Ws: epoch 196, train err 20.084, val err 18.436\n",
      "Info of returned Ws: epoch 197, train err 20.084, val err 18.994\n",
      "Info of returned Ws: epoch 198, train err 20.084, val err 18.994\n",
      "Info of returned Ws: epoch 199, train err 20.084, val err 18.994\n",
      "Info of returned Ws: epoch 200, train err 19.944, val err 18.994\n",
      "Info of returned Ws: epoch 201, train err 19.944, val err 19.553\n",
      "Info of returned Ws: epoch 202, train err 19.944, val err 19.553\n",
      "Info of returned Ws: epoch 203, train err 19.944, val err 19.553\n",
      "Info of returned Ws: epoch 204, train err 19.944, val err 19.553\n",
      "Info of returned Ws: epoch 205, train err 19.944, val err 19.553\n",
      "Info of returned Ws: epoch 206, train err 19.944, val err 19.553\n",
      "Info of returned Ws: epoch 207, train err 19.944, val err 19.553\n",
      "Info of returned Ws: epoch 208, train err 19.944, val err 19.553\n",
      "Info of returned Ws: epoch 209, train err 19.944, val err 19.553\n",
      "Info of returned Ws: epoch 210, train err 19.944, val err 19.553\n",
      "Info of returned Ws: epoch 211, train err 19.944, val err 19.553\n",
      "Info of returned Ws: epoch 212, train err 20.084, val err 19.553\n",
      "Info of returned Ws: epoch 213, train err 20.084, val err 19.553\n",
      "Info of returned Ws: epoch 214, train err 20.084, val err 19.553\n",
      "Info of returned Ws: epoch 215, train err 20.084, val err 19.553\n",
      "Info of returned Ws: epoch 216, train err 20.084, val err 19.553\n",
      "Info of returned Ws: epoch 217, train err 20.084, val err 19.553\n",
      "Info of returned Ws: epoch 218, train err 20.084, val err 19.553\n",
      "Info of returned Ws: epoch 219, train err 20.084, val err 19.553\n",
      "Info of returned Ws: epoch 220, train err 20.084, val err 19.553\n",
      "Info of returned Ws: epoch 221, train err 20.084, val err 19.553\n",
      "Info of returned Ws: epoch 222, train err 20.084, val err 19.553\n",
      "Info of returned Ws: epoch 223, train err 20.084, val err 19.553\n",
      "Info of returned Ws: epoch 224, train err 20.084, val err 19.553\n",
      "Info of returned Ws: epoch 225, train err 20.084, val err 19.553\n",
      "Info of returned Ws: epoch 226, train err 20.225, val err 19.553\n",
      "Info of returned Ws: epoch 227, train err 20.225, val err 19.553\n",
      "Info of returned Ws: epoch 228, train err 20.225, val err 19.553\n",
      "Info of returned Ws: epoch 229, train err 20.225, val err 19.553\n",
      "Info of returned Ws: epoch 230, train err 20.225, val err 19.553\n",
      "Info of returned Ws: epoch 231, train err 20.225, val err 19.553\n",
      "Info of returned Ws: epoch 232, train err 20.225, val err 19.553\n",
      "Info of returned Ws: epoch 233, train err 20.225, val err 19.553\n",
      "Info of returned Ws: epoch 234, train err 20.225, val err 19.553\n",
      "Info of returned Ws: epoch 235, train err 20.225, val err 19.553\n",
      "Info of returned Ws: epoch 236, train err 20.225, val err 19.553\n",
      "Info of returned Ws: epoch 237, train err 20.225, val err 19.553\n",
      "Info of returned Ws: epoch 238, train err 20.225, val err 19.553\n",
      "Info of returned Ws: epoch 239, train err 20.225, val err 19.553\n",
      "Info of returned Ws: epoch 240, train err 20.225, val err 19.553\n",
      "Info of returned Ws: epoch 241, train err 20.225, val err 19.553\n",
      "Info of returned Ws: epoch 242, train err 20.225, val err 19.553\n",
      "Info of returned Ws: epoch 243, train err 20.225, val err 19.553\n",
      "Info of returned Ws: epoch 244, train err 20.084, val err 19.553\n",
      "Info of returned Ws: epoch 245, train err 20.084, val err 19.553\n",
      "Info of returned Ws: epoch 246, train err 19.944, val err 19.553\n",
      "Info of returned Ws: epoch 247, train err 19.944, val err 19.553\n",
      "Info of returned Ws: epoch 248, train err 19.944, val err 19.553\n",
      "Info of returned Ws: epoch 249, train err 19.944, val err 19.553\n",
      "Info of returned Ws: epoch 250, train err 19.944, val err 19.553\n",
      "Info of returned Ws: epoch 251, train err 19.944, val err 19.553\n",
      "Info of returned Ws: epoch 252, train err 19.944, val err 19.553\n",
      "Info of returned Ws: epoch 253, train err 19.944, val err 19.553\n",
      "Info of returned Ws: epoch 254, train err 19.944, val err 19.553\n",
      "Info of returned Ws: epoch 255, train err 19.944, val err 18.994\n",
      "Info of returned Ws: epoch 256, train err 19.944, val err 18.994\n",
      "Info of returned Ws: epoch 257, train err 19.803, val err 18.994\n",
      "Info of returned Ws: epoch 258, train err 19.803, val err 18.994\n",
      "Info of returned Ws: epoch 259, train err 19.944, val err 18.994\n",
      "Info of returned Ws: epoch 260, train err 19.944, val err 18.994\n",
      "Info of returned Ws: epoch 261, train err 19.944, val err 18.994\n",
      "Info of returned Ws: epoch 262, train err 19.944, val err 18.994\n",
      "Info of returned Ws: epoch 263, train err 19.944, val err 18.994\n",
      "Info of returned Ws: epoch 264, train err 19.944, val err 18.994\n",
      "Info of returned Ws: epoch 265, train err 19.944, val err 18.994\n",
      "Info of returned Ws: epoch 266, train err 19.944, val err 18.994\n",
      "Info of returned Ws: epoch 267, train err 19.663, val err 18.994\n",
      "Info of returned Ws: epoch 268, train err 19.663, val err 18.994\n",
      "Info of returned Ws: epoch 269, train err 19.663, val err 18.994\n",
      "Info of returned Ws: epoch 270, train err 19.663, val err 18.994\n",
      "Info of returned Ws: epoch 271, train err 19.663, val err 18.994\n",
      "Info of returned Ws: epoch 272, train err 19.663, val err 18.994\n"
     ]
    }
   ],
   "source": [
    "Ws_0, train_errs_0, val_errs_0 = train_nnet(train_X, train_Y, val_X, val_Y, hid_layer_sizes=[50], \n",
    "                                            wd_level=0.0, mb_size=len(train_X), learning_rate=0.1, \n",
    "                                            max_patience=None, max_epoch=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kết quả của mình: \n",
    "\n",
    "`Info of returned Ws: epoch 7999, train err 15.730, val err 19.553`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Ws_1, train_errs_1, val_errs_1 = train_nnet(train_X, train_Y, val_X, val_Y, hid_layer_sizes=[50], \n",
    "                                            wd_level=0.001, mb_size=len(train_X), learning_rate=0.1, \n",
    "                                            max_patience=None, max_epoch=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kết quả của mình: \n",
    "\n",
    "`Info of returned Ws: epoch 7999, train err 17.697, val err 17.877`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Ws_2, train_errs_2, val_errs_2 = train_nnet(train_X, train_Y, val_X, val_Y, hid_layer_sizes=[50], \n",
    "                                            wd_level=0.005, mb_size=len(train_X), learning_rate=0.1, \n",
    "                                            max_patience=None, max_epoch=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kết quả của mình: \n",
    "\n",
    "`Info of returned Ws: epoch 7999, train err 19.242, val err 19.553`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "\n",
    "plt.figure(figsize=(14,10))\n",
    "num_epochs = len(train_errs_0)\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.plot(range(num_epochs), train_errs_0, 'r', label='wd_level 0.000, train err')\n",
    "plt.plot(range(num_epochs), val_errs_0, 'r--', label='wd_level 0.000, val err')\n",
    "plt.xlabel('num epochs')\n",
    "plt.ylabel('error')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.plot(range(num_epochs), train_errs_1, 'g', label='wd_level 0.001, train err')\n",
    "plt.plot(range(num_epochs), val_errs_1, 'g--', label='wd_level 0.001, val err')\n",
    "plt.xlabel('num epochs')\n",
    "plt.ylabel('error')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.plot(range(num_epochs), train_errs_2, 'b', label='wd_level 0.005, train err')\n",
    "plt.plot(range(num_epochs), val_errs_2, 'b--', label='wd_level 0.005, val err')\n",
    "plt.xlabel('num epochs')\n",
    "plt.ylabel('error')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.plot(range(num_epochs), train_errs_0, 'r', label='wd_level 0.000, train err')\n",
    "plt.plot(range(num_epochs), val_errs_0, 'r--', label='wd_level 0.000, val err')\n",
    "plt.plot(range(num_epochs), train_errs_1, 'g', label='wd_level 0.001, train err')\n",
    "plt.plot(range(num_epochs), val_errs_1, 'g--', label='wd_level 0.001, val err')\n",
    "plt.plot(range(num_epochs), train_errs_2, 'b', label='wd_level 0.005, train err')\n",
    "plt.plot(range(num_epochs), val_errs_2, 'b--', label='wd_level 0.005, val err')\n",
    "plt.xlabel('num epochs')\n",
    "plt.ylabel('error')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mình có đính kèm kết quả chạy của mình trong file `weight_decay.png`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Khi wd_decay = 0, ta nhận thấy rằng độ lỗi tập train có những đoạn rất thấp, nhưng độ lỗi trên tập validation lúc này lớn hơn rất nhiều. Chứng tỏ rằng đã xảy ra overfitting.\n",
    "+ Tương tự, khi wd_decay = 0.001, độ lỗi tập train có khác biệt so với trường hợp wd_decay = 0, nhưng vẫn xảy ra overfitting, khi độ lỗi trên tập validation lớn và tập train nhỏ.\n",
    "+ Khi wd_decay = 0.005, thấy được 2 đường độ lỗi trên tập train và tập validation xấp xỉ, đều nha, thể hiện không xảy ra overfitting trong trường hợp này."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thí nghiệm 2: ảnh hưởng của early stopping**\n",
    "\n",
    "Để thấy được ảnh hưởng của early stopping tới quá trình học, ta sẽ huấn luyện Neural Net với `max_patience=5000`; ở đây, ta cố định `hid_layer_sizes=[50], wd_level=0.0, mb_size=len(train_X), learning_rate=0.1, max_epoch=1000000(default)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Ws_3, train_errs_3, val_errs_3 = train_nnet(train_X, train_Y, val_X, val_Y, hid_layer_sizes=[50], \n",
    "                                            wd_level=0.0, mb_size=len(train_X), learning_rate=0.1, \n",
    "                                            max_patience=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kết quả của mình: \n",
    "\n",
    "`Info of returned Ws: epoch 3900, train err 16.713, val err 17.318`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "num_epochs = len(train_errs_3)\n",
    "stop_epoch = np.argmin(val_errs_3)\n",
    "plt.plot(range(num_epochs), train_errs_3, 'r', label='train err')\n",
    "plt.plot(range(num_epochs), val_errs_3, 'r--', label='val err')\n",
    "min_y, max_y = plt.ylim()\n",
    "plt.plot([stop_epoch, stop_epoch], [min_y, max_y])\n",
    "plt.xlabel('num epochs')\n",
    "plt.ylabel('error')\n",
    "plt.legend(loc='best')\n",
    "plt.text(stop_epoch, (min_y + max_y) / 2, 'STOP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mình có đính kèm kết quả chạy của mình trong file `early_stopping.png`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ta thấy rằng, tại vị trí stopping độ lỗi của cả 2 tập đang có xu hướng giảm. Đặc biệt, ở các giai đoạn tiếp theo sau vị trí stopping, độ lỗi tập validation dần trở nên tăng lên. Do đó vị trí stopping là vị trí mà độ lỗi trên tập validation và tập train đều nhỏ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kiểm tra (test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ta sẽ chọn `Ws` có độ lỗi nhỏ nhất trên tập validation làm bộ tham số của hàm dự đoán sau cùng; đó chính là `Ws_3` (dùng early stopping). Tất nhiên, ta có thể thí nghiệm thêm bằng cách kết hợp cả weight decay và early stopping (trong thực tế, người ta vẫn thường làm vậy, và có thể sẽ cho kết quả tốt hơn so với chỉ sử dụng weight decay hoặc early stopping). Mình đã có thí nghiệm nhưng độ lỗi trên tập validation không thấp hơn `Ws_3`; do đó, ta vẫn sẽ dùng `Ws_3` làm bộ tham số cuối cùng.\n",
    "\n",
    "Để đánh giá chất lượng của `Ws_3`, ta sẽ tiến hành dự đoán với các véc-tơ đầu vào của tập kiểm tra, rồi submit kết quả dự đoán lên Kaggle để biết được độ chính xác."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Đọc dữ liệu tập kiểm tra vào data frame `test_input_df`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_input_df = pd.read_csv('test.csv', index_col=0)\n",
    "test_input_df.info()\n",
    "test_input_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Xây dựng `test_X` từ `test_input_df`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_X = process_new_input_df(test_input_df, dropped_cols, mean_mode_dict, X_mean, X_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Dự đoán nhãn lớp của test_X**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "preds = np.argmax(compute_nnet_outputs(Ws_3, test_X, False), axis=1)\n",
    "\n",
    "# Create a data frame with `PassengerId` column (ids of passengers in test set) \n",
    "# and `Survived` column (predictions); then write it to a csv file\n",
    "preds_df = pd.DataFrame(preds, index=test_input_df.index, columns=['Survived'])\n",
    "preds_df.head()\n",
    "preds_df.to_csv('preds.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "*TODO: submit file `preds.csv` lên [Kaggle](https://www.kaggle.com/c/titanic/submissions/attach), và ghi nhận lại độ chính xác.*"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "153px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
