{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='darkblue'>Artificial Neural Network (ANN)</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>Lý thuyết</font>\n",
    "Ở những chương đầu tiên trong lớp học này, chúng ta đã bắt đầu cuộc hành trình và đi qua các thuật toán học máy với neural thần kinh nhân tạo trong **chương 2, Training Simple Machine Learning Algorithms for Classification**. Neural thần kinh nhân tạo có thể được hình dung như các khối gạch được sử dụng để xây nên một mạng thần kinh nhân tạo đa lớp (Multi layers artificial NNs), cái mà chúng ta sẽ cùng đề cập tới trong chương này. \n",
    "\n",
    "### <font color='blue'>Mô hình hóa các bài toán phức tạp với ANN</font>\n",
    "Khái niệm cơ bản đằng sau các NN nhân tạo được xây dựng trên **các giả thuyết và mô hình về cách thức hoạt động của bộ não con người** để giải quyết các nhiệm vụ phức tạp. Mặc dù NN nhân tạo chỉ mới phổ biến trong những năm gần đây, nhưng những nền tảng khái niệm đầu tiên của NN được xây dựng vào những năm đầu 1940 khi Warren McCulloch và Walter Pitts lần đầu tiên mô tả lại cách mà các neural hoạt động. *{A logical calculus of the ideas immanent in nervous activity, W. S. McCulloch and W. Pitts. The Bulletin of Mathematical Biophysics, 5(4):115–133, 1943.}*\n",
    "\n",
    "Tuy nhiên, sau nhiều thập kỷ kể từ khi thực nghiệm mô hình neural nhân tạo đầu tiên được thực hiện *(Rosenblatt's perceptron vào năm 1950s)*. Nhiều nhà nghiên cứu và nghiên cứu sinh dần dần chán nản và mất đi hứng thú với NN vì không một ai vào lúc bấy giờ có thể cho ra một giải pháp tốt cho NN nhiều lớp. \n",
    "\n",
    "*Dành cho các đọc giả quan tâm đến lịch sử của Artificial Intelligence (AI), machine learning hay NN nói riêng, tôi khuyến khích các bạn đọc qua bài viết bằng tiếng Anh rất chi tiết về các giai đoạn này trên Wikipedia có tựa [AI winters](https://en.wikipedia.org/wiki/AI_winter).*\n",
    "\n",
    "Cuối cùng, vào năm 1986 khi D.E.Rumelhart, G.E. Hinton và R.J. Williams đã công bố thuật toán backpropagation để đào tạo NN hiệu quả hơn, chúng ta sẽ thảo luận chi tiết hơn sau trong chương này, đã đưa NN trở lại cuộc đua trong lĩnh vực trí nghiên cứu về trí tuệ nhân tạo cùng với sự phát triển vượt bậc của phần cứng (GPU, TPU) mà NN đã và đang ngày càng trở nên phổ biến hơn bao giờ hết. Từ đó dẫn đến sự ra đời của khái niệm kiến trúc hay giải thuật **deep learning** mà chúng ta vẫn thường hay ghe tới. Một chủ đề nóng không chỉ trong giới nghiên cứu học thuật mà còn được đầu tư một khoản không nhỏ từ các ông lớn trong ngành công nghệ như Facebook, Microsoft, Amazon, Uber, và Google.\n",
    "\n",
    "Cho đến ngày nay, các NN phức tạp được hỗ trợ bởi các thuật toán học sâu được coi là giải pháp tiên tiến (SOTA) để giải quyết vấn đề phức tạp như nhận dạng hình ảnh, giọng nói hay xử lý ngôn ngữ. Các ví dụ phổ biến về các sản phẩm trong cuộc sống hàng ngày của chúng ta được hỗ trợ bởi các NN học sâu là tìm kiếm hình ảnh của Google và Google Dịch - một ứng dụng cho điện thoại thông minh có thể tự động nhận dạng văn bản trong hình ảnh để dịch theo thời gian thực sang hơn 20 ngôn ngữ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>Nhắc lại về mạng neural một lớp</font>\n",
    "Trước khi đi chi tiết hơn về kiên trúc NN đa lớp, hãy cùng nhặc lại một vài khái niệm về NN một lớp mà chúng ta đã được giới thiệu trong chương 2, Training Simple Machine Learning Algorithms for\n",
    "Classification, có tên gọi là giải thuật **ADAptive LInear NEuron (Adaline)**.\n",
    "![hinh 1](https://raw.githubusercontent.com/HuangRihChang/machine_learning_basic/master/ANNs/images/SLNN.png)\n",
    "<div align=\"center\">hình 1. kiến trúc của mạng neural nhân tạo một lớp</div>  \n",
    "\n",
    "Trong chương 2, chúng ta đã khai triển thuật toán Adaline để thực hiện tác vụ phân lớp nhị phân, và sử dụng thuật toán gradient descent optimization để cập nhật các hệ số trọng số của mô hình trong qua training set. Trong mỗi epoch, chúng ta cập nhật vector trọng số $w$ bằng cách sử dụng quy tắc cập nhật sau: \n",
    "$$\n",
    "\\begin{aligned}\n",
    " w:&=w+\\Delta{w}, \\text{    where $\\Delta{w} = -\\eta\\nabla{J(w)}$}\n",
    "\\end{aligned}\n",
    "$$\n",
    "nói cách khác, chúng ta đã thực hiện tính toán gradient dựa trên toàn bộ dữ liệu trong training set và cập nhật trọng số của mô hình bằng cách bước một bước về hướng ngược lại so với gradient (có thể hiểu là độ dốc) của $\\nabla{J(w)}$. Để tối ưu trọng số của mô hình, chúng ta tối ưu một objective function được xác định trước, ở đây chúng ta đã dùng Sum of Squared errors (SSE) để làm cost function $J(w)$. Tiếp đến, chúng ta còn định nghĩa learning rate dùng để cân bằng tốc độ học tập và tránh nguy cơ vượt qua Global Minimum của cost function.\n",
    "\n",
    "Trong gradient descent optimization, chúng ta cập nhật đồng thời tất cả các trọng số sau mỗi epoch, và chúng ta cũng xác định đạo hàm riêng của mỗi trọng số $w_j$ với công thức sau:\n",
    "$$\n",
    "\\begin{aligned}\n",
    " \\frac{\\partial}{\\partial{w_j}}J(w)&=-\\sum_i(y^{(i)}-a^{(i)})x^{(i)}_j\n",
    "\\end{aligned}\n",
    "$$\n",
    "Trong đó, $y^{(i)}$ là nhãn của lớp mục tiêu của mẫu $x^{(i)}$, và $a^{(i)}$ là hàm kích hoạt của neural.\n",
    "\n",
    "Xa hơn nữa, chúng ta đã định nghĩa hàm activation $\\phi(.)$ như sau:\n",
    "$$\n",
    "\\begin{aligned}\n",
    " \\phi(z) = z = a\n",
    "\\end{aligned}\n",
    "$$\n",
    "Trong đó, input $z$ là một tổ hợp tuyến tính (linear combination) có trọng số trên các cạnh nối giữa input layer và output layer:\n",
    "$$\n",
    "\\begin{aligned}\n",
    " z = \\sum_j w_j x_j = W^TX\n",
    "\\end{aligned}\n",
    "$$\n",
    "Trong khi chúng ta sử dụng activation $\\phi(z)$ để thực hiện tính toán cập nhật gradient, chúng ta còn xác định threshold để chuyển kết quả đầu ra từ giá trị trong miền liên tục qua thành giá trị nhị phân cho bài toán phân lớp nhị phân:\n",
    "$$\n",
    "\\begin{aligned}\n",
    " y = \\begin{cases}\n",
    "        1 \\: \\text{if $g(z)\\geq 0$}\\\\\n",
    "        -1 \\: \\text{otherwise}\n",
    "     \\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n",
    "Đồng thời, chúng ta cũng đã được học về một mẹo để tăng tốc quá trình học tập của mô hình, được gọi **stochastic gradient descent (SGD)** optimization. SGD xấp xỉ chi phí từ một mẫu huấn luyện (online learning) hoặc từ một tập hợp nhỏ (subset) được lấy từ mẫu huấn luyện (mini-batch learning). Ngoài việc giúp mô hình học tập nhanh hơn, do việc cập nhật weights thường xuyên hơn so với GD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>Giới thiệu kiến trúc mạng neural đa lớp</font>\n",
    "Trong phần này, chúng ta sẽ tìm hiểu cách kết nối nhiều neual đơn lẻ với một NN đa lớp; loại mạng đặc biệt này được gọi là Multi-Layer Perceptron (MLP). Hình dưới đây minh họa concept của một MLP gồm có ba lớp:\n",
    "![](https://raw.githubusercontent.com/HuangRihChang/machine_learning_basic/master/ANNs/images/FF.png)\n",
    "<div align=\"center\">hình 2. kiến trúc của mạng neural nhân tạo đa lớp</div>  \n",
    "\n",
    "MLP được mô tả ở hình trên bao gồm có một lớp đầu vào (Input layer), một lớp ẩn (hidden layer) và một lớp đầu ra (output layer). Các units ở hidden layer đều liên kết đầy đủ (fully connected) với input layer, tương tự với output layer cũng được fully connected với hidden layer trước đó. Nếu như một mạng như vậy có hơn 1 hidden layer, chúng ta gọi đó **deep artificial NN**.\n",
    "\n",
    "Như được thể hiện ở hình trước, chúng ta biểu diễn activation thứ $i$ ở lớp thứ $l$ bằng ký hiệu $a^{(l)}_i$. Để khiến cho việc triển khai toán học và coding thực nghiệm dễ dàng hơn, chúng ta sẽ không sử dụng các chỉ số bằng số để chỉ đến các lớp hay vị trí của các unit trong lớp. Thay vào đó chúng ta định nghĩa các superscript như *in* để chỉ input layer, *h* cho hidden layers, *out* cho output layer. Chú ý, để dễ dàng hơn cho việc định coding chúng ta chú ý các activation unit $a^{(in)}_0$ và $a^{(h)}_0$ là các bias units, được cài đặt mặc định bằng 1:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f&=  \\begin{bmatrix}\n",
    "        a^{(in)}_0\\\\\n",
    "        a^{(in)}_1\\\\\n",
    "        \\vdots\\\\\n",
    "        a^{(in)}_m \n",
    "    \\end{bmatrix} &= \\begin{bmatrix}\n",
    "        1\\\\\n",
    "        x^{(in)}_1\\\\\n",
    "        \\vdots\\\\\n",
    "        x^{(in)}_m \n",
    "    \\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Mỗi unit ở layer $l$ được liên kết với tất cả units ở layer $l+1$ bởi một bộ trọng số. Ví dụ, liên kết giữa unit thứ $k$ ở lớp $l$ và unit thứ $j$ ở lớp $l+1$, được ký hiệu bằng $w^{(l)}_{k,j}$. Tham khảo lại hình trước, chúng ta ký hiệu ma trận trọng số liên kết giữa input layer và hidden layer bằng $W^{(h)}$, và ma trận trọng số liên kết giữa hidden layer và output layer là $W^{(out)}$.\n",
    "\n",
    "Nhìn lại ở mô hình nhân tạo 1 lớp chúng ta thấy, trong khi với 1 unit ở output layer sẽ phù hợp với bài toán phân lớp nhị phân. Ở hình 2 chúng ta được cho thấy một dạng NN tổng quát hơn, cho phép chúng ta thực hiện phân loại đa lớp thông qua việc khái quát hóa kỹ thuật One-vs-All (OvA). Để hiểu rõ hơn về cách hoạt động của nó, chúng ta hãy cùng nhớ về cách biểu diễn one-hot đã được giới thiệu trong Chương 4, Building Good Training Datasets-Data Preprocessing.\n",
    "Ví dụ như chúng ta có 3 nhãn có được định nghĩa $[0,1,2]$, thông qua one-hot encode chúng ta sẽ có được biểu diễn sau:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "0&=  \\begin{bmatrix}\n",
    "        1\\\\\n",
    "        0\\\\\n",
    "        0 \n",
    "    \\end{bmatrix}, 1 =\\begin{bmatrix}\n",
    "        0\\\\\n",
    "        1\\\\\n",
    "        0 \n",
    "    \\end{bmatrix}, 2 =\\begin{bmatrix}\n",
    "        0\\\\\n",
    "        0\\\\\n",
    "        1\n",
    "    \\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "OKay, vậy là với NN đa lớp và với one-hot encoding chúng ta có thể định nghĩa được bài toán phân loại đa lớp với giả định là mỗi một điểm dữ liệu cụ thể chỉ thuộc một lớp duy nhất.\n",
    "\n",
    "Bây giờ chúng ta hay tóm tắt lại các khái niệm mà đã được quy ước trong phần này với biểu diễn trực quan hóa của 3-4-3 MLP nhé:\n",
    "\n",
    "![hinh 3](https://raw.githubusercontent.com/HuangRihChang/machine_learning_basic/master/ANNs/images/FF2.png)\n",
    "<div align=\"center\">hình 3. Minh họa quy ước trong NN đa lớp</div>\n",
    "\n",
    "Sau khi đã biết được các khái niệm cơ bản về MLP, chúng ta cùng tìm hiểu được làm thế nào một MLP có thể học được, tương tự như ở mạng neural 1 lớp, chúng ta tóm tắt với 3 bước sau:\n",
    "- bắt đầu ở input layer, chúng ta thực hiện lan truyền tiến (feed forward hay còn có tài liệu gọi là forward propagation) những patterns của training data trên kiến trúc MLP hiện thời để tính ra được output.\n",
    "- dựa trên output của network, chúng ta thực hiện tính toán độ lỗi (error) cần cực tiểu hóa thông qua cost function (có tài liệu sẽ gọi là lost function).\n",
    "- Thực hiện quá trình lan truyền ngược (backpropagation) độ lỗi, dựa trên đó tìm đạo hàm riêng của từng trọng số trong network, và cập nhật mô hình."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>Kích hoạt một NN thông qua lan truyền tiến (feed forward)</font>\n",
    "Trong phần này chúng ta sẽ cùng tìm hiểu về quá trình của quá trình feed forward để tính toán output của một mô hình MLP. \n",
    "\n",
    "Bây giờ chúng ta hãy cùng đi qua từng bước một để hiểu hơn về quá trình feed forward để tính được output. Vì mỗi unit trong hidden layer luôn được fully connected với tất cả các units ở lớp trước đó nên chúng ta tính kết quả của activation unit $a^{(h)}_1$ như sau:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "z^{(h)}_1 &= a^{(in)}_0 w^{(h)}_{0,1} + a^{(in)}_1 w^{(h)}_{1,1}+...+a^{(in)}_m w^{(h)}_{m,1} \\\\\n",
    "a^{(h)}_1 &= \\phi(z^{(h)}_1)\n",
    "\\end{aligned}\n",
    "$$\n",
    "Trong đó, $z^{(h)}_1$ là input của unit và $\\phi(.)$ là một activation function. Để có thể xử lý được các tác vụ phức tạp như là phân loại ảnh, hay xử lý giọng nói. Chúng ta sử dụng các hàm phi tuyến tính để làm các activation functions. Ví dụ như là hàm sigmoid đã được ví dụ ở Chương 3, A Tour of Machine Learning Classifiers Using scikit-learn:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\phi(z) = \\frac{1}{1+e^{-z}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Nhằm mục đích thuận tiện và hiệu quả trong việc code và dễ đọc, chúng ta sẽ viết lại hàm kích hoạt trên ở dạng ngắn gọn hơn bằng việc tận dụng các khái niệm trong đại số tuyến tính, cho phép chúng ta vetor hóa thông qua việc lập trình bằng numpy, thay vì viết hàng chục, hàng tỉ phép tính với python loops:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{z}^{(h)} &= \\boldsymbol{a}^{(in)}W^{(h)}\\\\\n",
    "\\boldsymbol{a}^{(h)} &= \\phi(\\boldsymbol{z}^{(h)})\n",
    "\\end{aligned}\n",
    "$$\n",
    "trong đó, $\\boldsymbol{a}^{(in)}$ là một ma trận đặc trưng của training sample có $1 \\,\\text{x}\\,m$ chiều. $W^{(h)}$ là một ma trận trọng số có $m \\, \\text{x} \\,d$ chiều. Với $m$ là số chiều đặc trưng input, $d$ là số units ở hidden layer. Sau khi thực hiện phép nhân ma trận, chúng ta thu được ma trận có $1\\,\\text{x}\\,d$ chiều dùng để làm giá trị đầu vào ròng (net input) để tính activation $\\boldsymbol{a}^{(h)}$ ($\\boldsymbol{a}^{(h)}\\in /R^{1\\,\\text{x}\\,d}$).\n",
    "\n",
    "Xa hơn nữa, chúng ta sẽ tổng quát hóa toàn bộ $n$ samples trong tập training set:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{Z}^{(h)} &= \\boldsymbol{A}^{(in)}W^{(h)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "Trong đó, $\\boldsymbol{A}^{(in)}$ là một ma trận $n \\text{x} m$ chiều, sau khi thực hiện phép nhân ma trận với ma trận như trên ta thu được một ma trận $\\boldsymbol{Z}^{(h)}$ có $n\\,\\text{x}\\,d$ chiều. Cuối cùng, chúng ta áp dụng activation function $\\phi(.)$ cho ma trận giá trị đầu vào ròng vừa thu được ở bước trên để tạo ra một ma trận activation mới cùng số chiều với ma trận đầu vào ròng:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{A}^{(h)} &= \\phi(\\boldsymbol{Z}^{(h)})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Tương tự với output layer,ta có công thức sau cho output layer dưới dạng ma trận hóa:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{Z}^{(out)} &= \\boldsymbol{A}^{(h)}W^{(out)}\\\\\n",
    "\\boldsymbol{A}^{(out)} &= \\phi(\\boldsymbol{Z}^{(out)}), \\,\\,A^{(out)}\\in\\mathbb{R}^{n\\,\\text{x}\\,t}\n",
    "\\end{aligned}\n",
    "$$\n",
    "Với $t$ là số output units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>Đôi chút về lan truyền ngược (backpropagation)</font>\n",
    "Mặc dù đã được (tái) khám phá và công bố hơn 30 năm qua *(Learning representations by back-propagating errors, D. E. Rumelhart, G. E. Hinton, and R. J. Williams, Nature, 323: 6088, pages 533–536, 1986)* nhưng cho đến nay, Backpropagation vẫn là một giải thuật được sử dụng chủ yếu và rộng rãi trong việc huấn luyện mạng trí tuệ nhân tạo hiệu quả nhất. \n",
    "\n",
    "Trong phần này chúng ta sẽ cùng nhắc lại một cách ngắn gọn và dễ hiểu về giải thuật này. Về bản chất, chúng ta có thể hiểu backpropagation như một cách tính toán hiệu quả cho việc tìm đạo hàm riêng của Loss Function phức tạp trong NN đa lớp. Ở đây, mục tiêu của chúng ta là sử dụng các đạo hàm riêng đó để học (cập nhật) các trọng số trong mô hình NN. Thách thức trong việc tham số hóa NN là chúng ta thường xử lý một số lượng lớn các hệ số có số chiều rất lớn. Khác với các Loss Function mà ta từng thấy ở NN 1 lớp như Adaline hay Logistic Regression, mặt lỗi (error surface) của NN đa lớp phần lớn đều không lồi, trơn và có rất nhiều các cực tiểu (local minimum) cần phải vượt qua để tìm ra giá trị nhỏ nhất (global minimum) của Loss Function.\n",
    "\n",
    "Đầu tiên, chúng ta sẽ cùng nhắc lại đôi chút về Quy tắc dây chuyền (Chain Rule) trong giải tích. Chain rule là hướng tiếp cận để giải quyết bài toán tìm đạo hàm riêng (Partial derivative) của một hàm phức tạp (complex function), hàm lồng nhau (nested function), ví dụ như sau:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial{x}}[h(f(x))] = \\frac{\\partial{h}}{\\partial{f}} \\cdot \\frac{\\partial f}{\\partial x}\n",
    "\\end{aligned}\n",
    "$$\n",
    "Tương tự vậy, chúng ta có thể áp dụng chain rule cho những hàm phức tạp hơn. Ví dụ như sau:\n",
    "\n",
    "Giả sử ta có 5 hàm khác nhau: $f(.),g(.),h(.),u(.)\\,\\text{ và }\\,v(x)$. Gọi $F$ là hàm hợp của 5 hàm trên: $F(x) = f(g(h(u(v(x)))))$. Áp dụng chain rule để tìm đạo hàm riêng của $x$ với $F$ ta được phương trình sau:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial F}{\\partial x} = \\frac{\\partial}{\\partial x}f(g(h(u(v(x))))) &= \\frac{\\partial f}{\\partial g} \\cdot \\frac{\\partial g}{\\partial h} \\cdot \\frac{\\partial h}{\\partial u} \\cdot \\frac{\\partial u}{\\partial v} \\cdot \\frac{\\partial v}{\\partial x}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "***Đọc thêm:***\n",
    "\n",
    "*Đạo hàm riêng tự động (Automatic differentiation)*: [**A. G. Baydin and B. A. Pearlmutter's article Automatic Differentiation of Algorithms for Machine Learning**, *arXiv preprint arXiv:1404.7456, 2014.*](http://arxiv.org/pdf/1404.7456.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>Huấn luyện NNs thông qua backpropagation</font>\n",
    "Trong phần này, chúng ta sẽ cùng nói một chút toán học về backpropagation để hiểu hơn về các mà NNs học (cập nhật) được các trọng số (weights) trong mô hình. \n",
    "\n",
    "Trước tiên, để tính được Loss và áp dụng backpropagation, chúng ta cần đi qua bước feed forward tuần tự các lớp để thu được đầu ra tại output, được mô hình hóa toán học qua như sau:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{Z}^{(h)} &= \\boldsymbol{A}^{(in)}W^{(h)}\\,\\,\\text{(net input of the hidden layer) (1)}\\\\\n",
    "\\boldsymbol{A}^{(h)} &= \\phi(\\boldsymbol{Z}^{(h)}) \\,\\,\\text{(activation of the hidden layer) (2)}\\\\\n",
    "\\boldsymbol{Z}^{(out)} &= \\boldsymbol{A}^{(h)}W^{(out)}\\,\\,\\text{(net input of the output layer) (3)}\\\\\n",
    "\\boldsymbol{A}^{(out)} &= \\phi(\\boldsymbol{Z}^{(out)}) \\,\\,\\text{(activation of the output layer) (4)}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Hình ảnh được mô tả với hình dưới đây:\n",
    "![](https://raw.githubusercontent.com/HuangRihChang/machine_learning_basic/master/ANNs/images/FF3.png)\n",
    "Trong quá trình backpropagation, chúng ta sẽ thực hiện lan truyền ngược độ lỗi (error) từ phải sáng trái (output layer - inputlayer). \n",
    "Dựa trên chain rule, ta có:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial J(\\boldsymbol{W})}{\\partial{\\boldsymbol{W}^{(h)}}} &= \\frac{\\partial \\boldsymbol{Z}^{(h)}}{\\partial \\boldsymbol{W}^{(h)}} \\frac{\\partial \\boldsymbol{A}^{(h)}}{\\partial \\boldsymbol{Z}^{(h)}} \\frac{\\partial J(\\boldsymbol{W})}{\\partial \\boldsymbol{A}^{(h)}}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "Dựa trên các phương trình (1) chúng ra suy ra:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\boldsymbol{Z}^{(h)}}{\\partial \\boldsymbol{W}^{(h)}} = \\boldsymbol{A}^{(in)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Đặt $\\frac{\\partial \\boldsymbol{A}^{(h)}}{\\partial \\boldsymbol{Z}^{(h)}} \\frac{\\partial J(\\boldsymbol{W})}{\\partial \\boldsymbol{A}^{(h)}} = \\delta^{(h)}$, Tiếp tục triển khai:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\delta^{(h)} = \\frac{\\partial \\boldsymbol{A}^{(h)}}{\\partial \\boldsymbol{Z}^{(h)}} \\frac{\\partial J(\\boldsymbol{W})}{\\partial \\boldsymbol{A}^{(h)}} =  \\frac{\\partial \\boldsymbol{A}^{(h)}}{\\partial \\boldsymbol{Z}^{(h)}} \\frac{\\partial \\boldsymbol{Z}^{(out)}}{\\partial \\boldsymbol{A}^{(h)}} \\frac{\\partial J(\\boldsymbol{W})}{\\partial \\boldsymbol{Z}^{(out)}} \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Chúng ta công thức sau:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\delta^{(h)} &= \\delta^{(out)}(\\boldsymbol{W}^{(out)})^T\\odot \\frac{\\partial \\phi{z^{(h)}}}{\\partial{z^{(h)}}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Với, $\\frac{\\partial \\phi{(z^{(h)})}}{\\partial{z^{(h)}}}$ là đạo hàm của activation function, cái chúng ta sẽ cùng tính một vài hàm activation cơ bản ở phần sau. Ở đây, để cho đơn giản, chúng ta giả sử activation function sẽ sử dụng là hàm sigmoid, có đạo hàm như sau:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial\\phi{(z)}}{\\partial{z}} = \\phi'(z) =(\\phi{(z)}\\odot(1-\\phi{(z)}))\n",
    "\\end{aligned}\n",
    "$$\n",
    "và  $\\delta^{(out)}$ ta tính bằng phương pháp (chứng minh ở [link]( https://math.stackexchange.com/questions/945871/derivative-of-softmax-loss-function) này):\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\delta^{(out)} = a^{(out)} - y\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "Trong đó, $y$ là ma trận giá trị nhãn (true class labels). \n",
    "\n",
    "\n",
    "Phép $\\odot$ được đề cập đến ở 2 công thức trên là phép nhân nguyên tố (element-wise multiplication).\n",
    "\n",
    "$\\delta^{(h)}$ sẽ thu được công thức sau:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\delta^{(h)} &= \\delta^{(out)}(\\boldsymbol{W}^{(out)})^T\\odot(a^{(h)}\\odot(1-a^{(h)}))\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Chúng ta có thể hình dung phép tính trên như sau: $(n \\text{,} h) = (n \\text{,} c)\\cdot(c \\text{,} h) \\odot (n \\text{,} h)$ với $n$ là số dữ liệu traning, $c$ là số classes, $h$ là số hidden activation units.\n",
    "\n",
    "Để dễ dàng thực nghiệm và lập trình, chúng ta vector hóa công thức dưới dạng sau:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\Delta^{(h)} = (\\boldsymbol{A}^{(in)})^T \\delta^{(h)}\\\\\n",
    "\\Delta^{(out)} = (\\boldsymbol{A}^{(h)})^{T} \\delta^{(out)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Cuối cùng, sau khi đã tính được gradients, chúng ta có thể cập nhật trọng số của mô hình ở mỗi lớp $l$ bằng cách đi ngược chiều của một step của gradient:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{W}^{(l)}:=\\boldsymbol{W}^{(l)}-\\eta\\Delta^{(l)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "![](https://raw.githubusercontent.com/HuangRihChang/machine_learning_basic/master/ANNs/images/BF.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>Thực hành Huấn luyện mạng trí tuệ nhân tạo:\n",
    "Trong bài này, chúng ta sẽ thực hành: \n",
    "- (i) tiền xử lý dữ liệu\n",
    "- (ii) huấn luyện Neural Net\n",
    "\n",
    "Bộ dữ liệu được sử dụng là bộ [Kaggle Titanic](https://www.kaggle.com/c/titanic); trong đó, đầu vào là thông tin của hành khách trên tàu Titanic (bạn xem chi tiết trong file `description.txt` đính kèm), đầu ra là một trong hai lớp sống/chết (1 | 0). Mình có đính kèm các file dữ liệu: `train.csv` - tập huấn luyện, `val.csv` - tập validation, `test.csv` - tập kiểm tra (chỉ có đầu vào); thật ra, Kaggle chỉ cung cấp hai file là `train.csv` và `test.csv`, mình đã tách file `train.csv` của Kaggle ra hai file là `train.csv` (80%) và `val.csv` (20%).\n",
    "\n",
    "### <font color='blue'>Import</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy, re, time\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "from tqdm.notebook import tqdm\n",
    "#import cPickle\n",
    "# You can also import other things ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>Đọc dữ liệu tập huấn luyện và tập validation vào data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 712 entries, 496 to 153\n",
      "Data columns (total 11 columns):\n",
      "Survived    712 non-null int64\n",
      "Pclass      712 non-null int64\n",
      "Name        712 non-null object\n",
      "Sex         712 non-null object\n",
      "Age         574 non-null float64\n",
      "SibSp       712 non-null int64\n",
      "Parch       712 non-null int64\n",
      "Ticket      712 non-null object\n",
      "Fare        712 non-null float64\n",
      "Cabin       160 non-null object\n",
      "Embarked    711 non-null object\n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 66.8+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>496</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Yousseff, Mr. Gerious</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2627</td>\n",
       "      <td>14.4583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>649</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Willey, Mr. Edward</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S.O./P.P. 751</td>\n",
       "      <td>7.5500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>279</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Rice, Master. Eric</td>\n",
       "      <td>male</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>382652</td>\n",
       "      <td>29.1250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Spencer, Mrs. William Augustus (Marie Eugenie)</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17569</td>\n",
       "      <td>146.5208</td>\n",
       "      <td>B78</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Touma, Mrs. Darwis (Hanne Youssef Razi)</td>\n",
       "      <td>female</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2650</td>\n",
       "      <td>15.2458</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Survived  Pclass                                            Name  \\\n",
       "PassengerId                                                                     \n",
       "496                 0       3                           Yousseff, Mr. Gerious   \n",
       "649                 0       3                              Willey, Mr. Edward   \n",
       "279                 0       3                              Rice, Master. Eric   \n",
       "32                  1       1  Spencer, Mrs. William Augustus (Marie Eugenie)   \n",
       "256                 1       3         Touma, Mrs. Darwis (Hanne Youssef Razi)   \n",
       "\n",
       "                Sex   Age  SibSp  Parch         Ticket      Fare Cabin  \\\n",
       "PassengerId                                                              \n",
       "496            male   NaN      0      0           2627   14.4583   NaN   \n",
       "649            male   NaN      0      0  S.O./P.P. 751    7.5500   NaN   \n",
       "279            male   7.0      4      1         382652   29.1250   NaN   \n",
       "32           female   NaN      1      0       PC 17569  146.5208   B78   \n",
       "256          female  29.0      0      2           2650   15.2458   NaN   \n",
       "\n",
       "            Embarked  \n",
       "PassengerId           \n",
       "496                C  \n",
       "649                S  \n",
       "279                Q  \n",
       "32                 C  \n",
       "256                C  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('train.csv', index_col=0)\n",
    "train_df.info()\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 179 entries, 529 to 685\n",
      "Data columns (total 11 columns):\n",
      "Survived    179 non-null int64\n",
      "Pclass      179 non-null int64\n",
      "Name        179 non-null object\n",
      "Sex         179 non-null object\n",
      "Age         140 non-null float64\n",
      "SibSp       179 non-null int64\n",
      "Parch       179 non-null int64\n",
      "Ticket      179 non-null object\n",
      "Fare        179 non-null float64\n",
      "Cabin       44 non-null object\n",
      "Embarked    178 non-null object\n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 16.8+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>866</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Bystrom, Mrs. (Karolina)</td>\n",
       "      <td>female</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>236852</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>257</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Thorne, Mrs. Gertrude Maybelle</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17585</td>\n",
       "      <td>79.2000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>O'Driscoll, Miss. Bridget</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14311</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>868</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Roebling, Mr. Washington Augustus II</td>\n",
       "      <td>male</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17590</td>\n",
       "      <td>50.4958</td>\n",
       "      <td>A24</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>266</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Reeves, Mr. David</td>\n",
       "      <td>male</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C.A. 17248</td>\n",
       "      <td>10.5000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Hocking, Mrs. Elizabeth (Eliza Needs)</td>\n",
       "      <td>female</td>\n",
       "      <td>54.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>29105</td>\n",
       "      <td>23.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Smith, Mr. James Clinch</td>\n",
       "      <td>male</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17764</td>\n",
       "      <td>30.6958</td>\n",
       "      <td>A7</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>399</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Pain, Dr. Alfred</td>\n",
       "      <td>male</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>244278</td>\n",
       "      <td>10.5000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>374</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Ringhini, Mr. Sante</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17760</td>\n",
       "      <td>135.6333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Pears, Mrs. Thomas (Edith Wearne)</td>\n",
       "      <td>female</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113776</td>\n",
       "      <td>66.6000</td>\n",
       "      <td>C2</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Survived  Pclass                                   Name     Sex  \\\n",
       "PassengerId                                                                    \n",
       "866                 1       2               Bystrom, Mrs. (Karolina)  female   \n",
       "257                 1       1         Thorne, Mrs. Gertrude Maybelle  female   \n",
       "48                  1       3              O'Driscoll, Miss. Bridget  female   \n",
       "868                 0       1   Roebling, Mr. Washington Augustus II    male   \n",
       "266                 0       2                      Reeves, Mr. David    male   \n",
       "775                 1       2  Hocking, Mrs. Elizabeth (Eliza Needs)  female   \n",
       "175                 0       1                Smith, Mr. James Clinch    male   \n",
       "399                 0       2                       Pain, Dr. Alfred    male   \n",
       "374                 0       1                    Ringhini, Mr. Sante    male   \n",
       "152                 1       1      Pears, Mrs. Thomas (Edith Wearne)  female   \n",
       "\n",
       "              Age  SibSp  Parch      Ticket      Fare Cabin Embarked  \n",
       "PassengerId                                                           \n",
       "866          42.0      0      0      236852   13.0000   NaN        S  \n",
       "257           NaN      0      0    PC 17585   79.2000   NaN        C  \n",
       "48            NaN      0      0       14311    7.7500   NaN        Q  \n",
       "868          31.0      0      0    PC 17590   50.4958   A24        S  \n",
       "266          36.0      0      0  C.A. 17248   10.5000   NaN        S  \n",
       "775          54.0      1      3       29105   23.0000   NaN        S  \n",
       "175          56.0      0      0       17764   30.6958    A7        C  \n",
       "399          23.0      0      0      244278   10.5000   NaN        S  \n",
       "374          22.0      0      0    PC 17760  135.6333   NaN        C  \n",
       "152          22.0      1      0      113776   66.6000    C2        S  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df = pd.read_csv('val.csv', index_col=0)\n",
    "val_df.info()\n",
    "val_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>Tiền xử lý"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Tiền xử lý tập huấn luyện**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_df = train_df.iloc[:, 1:]\n",
    "train_output_df = train_df.Survived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Xây dựng `train_X` từ `train_input_df`\n",
    "\n",
    "(`train_X` là mảng numpy chứa các véc-tơ đầu vào mà có thể đưa trực tiếp vào mô hình học như ở các bài tập trước.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đầu tiên, ta sẽ tiến hành: \n",
    "- Bỏ cột `Cabin` vì cột này có nhiều giá thiếu (552/712).\n",
    "- Bỏ cột `Ticket` vì cột này có giá trị không phải dạng số, sẽ cần phải tốn sức để chuyển sang dạng số , vì nó có quá nhiều định dạng và rất cần rất nhiều công sức và thời gian để có thể hiểu được các kí hiệu trên đó có nghĩa gì.\n",
    "\n",
    "**Lưu ý** :\n",
    "\n",
    "khi bỏ 2 cột `Cabin` và `Ticket` đi thì rất có thể sẽ làm mất thông tin để dự đoán `sống`|`chết`. Vì có khả năng đây là dữ liệu quan trọng liên quan đến vị trí nghỉ ngơi cụ thể trong từng khoang.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 712 entries, 496 to 153\n",
      "Data columns (total 8 columns):\n",
      "Pclass      712 non-null int64\n",
      "Name        712 non-null object\n",
      "Sex         712 non-null object\n",
      "Age         574 non-null float64\n",
      "SibSp       712 non-null int64\n",
      "Parch       712 non-null int64\n",
      "Fare        712 non-null float64\n",
      "Embarked    711 non-null object\n",
      "dtypes: float64(2), int64(3), object(3)\n",
      "memory usage: 50.1+ KB\n"
     ]
    }
   ],
   "source": [
    "dropped_cols = ['Ticket', 'Cabin']\n",
    "train_input_df.drop(dropped_cols, axis=1, inplace=True)\n",
    "train_input_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kế đến, ta sẽ xử lý các giá trị thiếu ở cột `Age` và cột `Embarked`. Ở đây, ta xử lý đơn giản là điền giá trị mean (giá trị trung bình) vào các giá trị thiếu ở cột `Age`, và điền giá trị mode (giá trị xuất hiện nhiều nhất) vào các giá trị thiếu ở cột `Embarked`. *Lưu ý là, khi điền các giá trị thiếu ở tập validation/test, ta sẽ sử dụng các giá trị mean/mode được tính từ tập huấn luyện*. Trong thực tế,  ta thường sẽ không biết ở thời điểm test biến đầu vào nào sẽ thiếu giá trị; do đó, ta có thể làm một cách tổng quát như sau: với biến đầu vào có giá trị số, ta sẽ điền giá trị mean; ngược lại, điền giá trị mode. Dưới đây, hàm `compute_mean_mode` sẽ tính từ tập huấn luyện giá trị mean/mode của *tất cả* các biến đầu vào; hàm `fill_missing_values` sẽ dùng các giá trị mean/mode này để điền giá trị thiếu cho một tập dữ liệu nào đó (tập huấn luyện/validation/kiểm tra)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "712"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = train_input_df[\"Name\"].to_list()\n",
    "len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' the Countess',\n",
       " ' Major',\n",
       " ' Col',\n",
       " ' Ms',\n",
       " ' Lady',\n",
       " ' Master',\n",
       " ' Mme',\n",
       " ' Capt',\n",
       " ' Miss',\n",
       " ' Mrs',\n",
       " ' Don',\n",
       " ' Mlle',\n",
       " ' Dr',\n",
       " ' Jonkheer',\n",
       " ' Mr',\n",
       " ' Rev']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apellation = set([name.split(',')[1].split('.')[0] for name in names])\n",
    "apellation = list(apellation)\n",
    "apellation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_Name(input_df):\n",
    "    index = input_df.index.values.tolist()\n",
    "    vector = []\n",
    "    for key in apellation:\n",
    "        tmp = list([])\n",
    "        for i in index:\n",
    "            apell = input_df.loc[i][\"Name\"].split(',')[1].split('.')[0]\n",
    "            if(apell == key):\n",
    "                tmp.append(1.0)\n",
    "            else:\n",
    "                tmp.append(0.0)\n",
    "        vector.append(tmp)\n",
    "        k = 6\n",
    "    for i in range(len(vector)):\n",
    "        input_df.insert(k+i, apellation[i], vector[i], allow_duplicates = True)\n",
    "    input_df.drop('Name', axis=1, inplace=True)\n",
    "    return input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_mode(train_input_df):\n",
    "    '''\n",
    "    Computes means for numeric input variables and modes for non-numeric ones.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    train_input_df : data frame\n",
    "        The data frame containing training inputs.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mean_mode_dict : dictionary, len = # input variables (# columns) of train_input_df\n",
    "        mean_mode_dict[<column_name>] = mean/mode of this column.\n",
    "    '''\n",
    "    dic = {}\n",
    "    for column in train_input_df:\n",
    "        num = 0\n",
    "        for i in range(7000):\n",
    "            try:\n",
    "                a = train_input_df.loc[i][column]\n",
    "                num = i\n",
    "                break\n",
    "            except:\n",
    "                next\n",
    "        if (isinstance(train_input_df.loc[num][column], str)):\n",
    "            if(len(train_input_df[column].mode()) > 0):\n",
    "                dic[column] = str(train_input_df[column].mode()[0])\n",
    "            else:\n",
    "                dic[column] = ''\n",
    "        else:\n",
    "            dic[column] = train_input_df[column].mean()\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Pclass': 2.3230337078651684,\n",
       " 'Name': 'Abbott, Mr. Rossmore Edward',\n",
       " 'Sex': 'male',\n",
       " 'Age': 29.488815331010457,\n",
       " 'SibSp': 0.5140449438202247,\n",
       " 'Parch': 0.37359550561797755,\n",
       " 'Fare': 32.18301095505614,\n",
       " 'Embarked': 'S'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_mode_dict = compute_mean_mode(train_input_df)\n",
    "mean_mode_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_values(input_df, mean_mode_dict):\n",
    "    '''\n",
    "    Fills missing values for ALL columns of `input_df` using `mean_mode_dict`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_df : data frame\n",
    "        The data frame containing inputs.\n",
    "    mean_mode_dict : dictionary\n",
    "        mean_mode_dict[<column_name>] = mean/mode of this column (estimated from the training set).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    filled_input_df : data frame\n",
    "        The data frame containing inputs after filling missing values.\n",
    "    '''\n",
    "    for column in input_df:\n",
    "        df = input_df[pd.isnull(input_df).any(axis=1)]\n",
    "        index = df.index.to_numpy()\n",
    "        for i in index:\n",
    "            if(pd.isnull(input_df.loc[i, column])):\n",
    "                input_df.loc[i, column] = mean_mode_dict[column]\n",
    "    return input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId\n",
      "496   NaN\n",
      "649   NaN\n",
      "32    NaN\n",
      "299   NaN\n",
      "368   NaN\n",
      "Name: Age, dtype: float64\n",
      "\n",
      "PassengerId\n",
      "62    NaN\n",
      "Name: Embarked, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Before filling\n",
    "missing_age_mask = train_input_df.Age.isnull()\n",
    "missing_embarked_mask = train_input_df.Embarked.isnull()\n",
    "print(train_input_df.Age[missing_age_mask].head())\n",
    "print()\n",
    "print(train_input_df.Embarked[missing_embarked_mask].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId\n",
      "496    29.488815\n",
      "649    29.488815\n",
      "32     29.488815\n",
      "299    29.488815\n",
      "368    29.488815\n",
      "Name: Age, dtype: float64\n",
      "\n",
      "PassengerId\n",
      "62    S\n",
      "Name: Embarked, dtype: object\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 712 entries, 496 to 153\n",
      "Data columns (total 8 columns):\n",
      "Pclass      712 non-null int64\n",
      "Name        712 non-null object\n",
      "Sex         712 non-null object\n",
      "Age         712 non-null float64\n",
      "SibSp       712 non-null int64\n",
      "Parch       712 non-null int64\n",
      "Fare        712 non-null float64\n",
      "Embarked    712 non-null object\n",
      "dtypes: float64(2), int64(3), object(3)\n",
      "memory usage: 70.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# Fill and check the result\n",
    "train_input_df = fill_missing_values(train_input_df, mean_mode_dict)\n",
    "print(train_input_df.Age[missing_age_mask].head())\n",
    "print()\n",
    "print(train_input_df.Embarked[missing_embarked_mask].head())\n",
    "print()\n",
    "train_input_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đến đây, ta đã bỏ các cột `Name`, `Ticket`, `Cabin`; và điền giá trị thiếu cho cột `Age`, `Embarked`. Kế đến, ta sẽ chuyển các cột có giá trị không phải dạng số (`Sex` và `Embarked`) sang dạng số. Vì `Sex` và `Embarked` là các biến có giá trị rời-rạc và không-có-thứ-tự nên ta có thể chuyển sang dạng \"one-hot\" (và bỏ cột cuối). Vd:\n",
    "\n",
    "```\n",
    "Sex    --> Female | Male\n",
    "------     --------------\n",
    "female --> 1      | 0\n",
    "male   --> 0      | 1\n",
    "female --> 1      | 0\n",
    "```\n",
    "và ta có thể bỏ cột `Male` đi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_categorical_cols(input_df):\n",
    "    '''\n",
    "    Converts `Sex` column and `Embarked` column to one-hot forms.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_df : data frame\n",
    "        The data frame containing inputs.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    numeric_input_df : data frame\n",
    "        The data frame containing inputs after converting.\n",
    "    '''\n",
    "    sex_tag = ['female', 'male']\n",
    "    Embarked_tag = ['C','Q','S']\n",
    "    index = input_df.index.to_numpy()\n",
    "    value_sex = []\n",
    "    value_C = []\n",
    "    value_Q = []\n",
    "    \n",
    "    print(len(input_df))\n",
    "    \n",
    "    try:\n",
    "        for i in range(len(index)):\n",
    "            if(input_df.loc[int(index[i])]['Sex'] == sex_tag[0]):\n",
    "                value_sex.append(1.0)\n",
    "            else:\n",
    "                value_sex.append(0.0)\n",
    "            if(input_df.loc[int(index[i])]['Embarked'] == Embarked_tag[0]):\n",
    "                value_C.append(1.0)\n",
    "            else:\n",
    "                value_C.append(0.0)\n",
    "            if(input_df.loc[int(index[i])]['Embarked'] == Embarked_tag[1]):\n",
    "                value_Q.append(1.0)\n",
    "            else:\n",
    "                value_Q.append(0.0)\n",
    "    \n",
    "        input_df.insert(6, 'female', value_sex, allow_duplicates = True)\n",
    "        input_df.insert(7, 'C', value_C, allow_duplicates = True)\n",
    "        input_df.insert(8, 'Q', value_Q, allow_duplicates = True)\n",
    "        input_df.drop('Sex', axis=1, inplace=True)\n",
    "        input_df.drop('Embarked', axis=1, inplace=True)\n",
    "    except:\n",
    "        return input_df\n",
    "    return input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "712\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 712 entries, 496 to 153\n",
      "Data columns (total 9 columns):\n",
      "Pclass    712 non-null int64\n",
      "Name      712 non-null object\n",
      "Age       712 non-null float64\n",
      "SibSp     712 non-null int64\n",
      "Parch     712 non-null int64\n",
      "female    712 non-null float64\n",
      "C         712 non-null float64\n",
      "Q         712 non-null float64\n",
      "Fare      712 non-null float64\n",
      "dtypes: float64(5), int64(3), object(1)\n",
      "memory usage: 75.6+ KB\n"
     ]
    }
   ],
   "source": [
    "train_input_df = process_categorical_cols(train_input_df)\n",
    "train_input_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 712 entries, 496 to 153\n",
      "Data columns (total 24 columns):\n",
      "Pclass           712 non-null int64\n",
      "Age              712 non-null float64\n",
      "SibSp            712 non-null int64\n",
      "Parch            712 non-null int64\n",
      "female           712 non-null float64\n",
      " the Countess    712 non-null float64\n",
      " Major           712 non-null float64\n",
      " Col             712 non-null float64\n",
      " Ms              712 non-null float64\n",
      " Lady            712 non-null float64\n",
      " Master          712 non-null float64\n",
      " Mme             712 non-null float64\n",
      " Capt            712 non-null float64\n",
      " Miss            712 non-null float64\n",
      " Mrs             712 non-null float64\n",
      " Don             712 non-null float64\n",
      " Mlle            712 non-null float64\n",
      " Dr              712 non-null float64\n",
      " Jonkheer        712 non-null float64\n",
      " Mr              712 non-null float64\n",
      " Rev             712 non-null float64\n",
      "C                712 non-null float64\n",
      "Q                712 non-null float64\n",
      "Fare             712 non-null float64\n",
      "dtypes: float64(21), int64(3)\n",
      "memory usage: 159.1 KB\n"
     ]
    }
   ],
   "source": [
    "train_input_df = preprocessing_Name(train_input_df)\n",
    "train_input_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuối cùng, ta sẽ xây dựng `train_X`. Để giúp Gradient Descent hội tụ nhanh hơn, ta sẽ chuẩn hóa để các cột của `train_X` có mean bằng 0 và có độ lệch chuẩn bằng 1. *Lưu ý là, khi chuẩn hóa `val_X`/`test_X`, ta sẽ dùng mean và độ lệch chuẩn được ước lượng từ tập huấn luyện.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(712, 24)\n"
     ]
    }
   ],
   "source": [
    "train_X = train_input_df.values\n",
    "print(train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute `X_mean` and `X_std`\n",
    "X_mean = train_X.mean(axis=0)\n",
    "X_std = train_X.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.32303371e+00 2.94888153e+01 5.14044944e-01 3.73595506e-01\n",
      " 3.49719101e-01 1.40449438e-03 2.80898876e-03 2.80898876e-03\n",
      " 1.40449438e-03 1.40449438e-03 4.63483146e-02 1.40449438e-03\n",
      " 1.40449438e-03 2.12078652e-01 1.27808989e-01 1.40449438e-03\n",
      " 2.80898876e-03 7.02247191e-03 1.40449438e-03 5.80056180e-01\n",
      " 8.42696629e-03 1.79775281e-01 8.42696629e-02 3.21830110e+01]\n",
      "[8.34392606e-01 1.31205457e+01 1.07438220e+00 8.00827868e-01\n",
      " 4.76881171e-01 3.74502574e-02 5.29254036e-02 5.29254036e-02\n",
      " 3.74502574e-02 3.74502574e-02 2.10238313e-01 3.74502574e-02\n",
      " 3.74502574e-02 4.08780255e-01 3.33877000e-01 3.74502574e-02\n",
      " 5.29254036e-02 8.35054298e-02 3.74502574e-02 4.93549398e-01\n",
      " 9.14108994e-02 3.84000168e-01 2.77791805e-01 5.22947658e+01]\n"
     ]
    }
   ],
   "source": [
    "print(X_mean)\n",
    "print(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize train_X using X_mean and X_std\n",
    "train_X = (train_X - X_mean) / X_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.09570189e-16 -4.98976640e-18  1.24744160e-18 -4.24130144e-17\n",
      "  6.48669632e-17  0.00000000e+00 -9.97953281e-18 -9.97953281e-18\n",
      "  2.49488320e-18  2.49488320e-18 -2.49488320e-18 -4.98976640e-18\n",
      "  2.49488320e-18  8.60734704e-17 -3.49283648e-17  2.49488320e-18\n",
      "  0.00000000e+00 -9.97953281e-18 -2.49488320e-18  3.99181312e-17\n",
      " -7.48464960e-18  2.49488320e-17  7.48464960e-18 -5.48874304e-17]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(train_X.mean(axis=0))\n",
    "print(train_X.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2. Xây dựng `train_Y` từ `train_output_df`**\n",
    "\n",
    "(`train_Y` là mảng numpy chứa các đầu ra đúng mà có thể đưa trực tiếp vào mô hình học như ở các bài tập trước.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(712, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "train_Y = train_output_df.values.reshape(-1, 1)\n",
    "enc.fit(train_Y)\n",
    "\n",
    "train_Y = enc.transform(train_Y).toarray()\n",
    "print(train_Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Tiền xử lý tập validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_input_df = val_df.iloc[:, 1:]\n",
    "val_output_df = val_df.Survived\n",
    "len(val_input_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1. Xây dựng `val_X` từ `val_input_df`**\n",
    "\n",
    "Cách xây dựng `test_X` từ `test_input_df` sẽ *giống hệt* cách xây dựng `val_X` từ `val_input_df`. Do đó, ta sẽ viết phần xử lý này vào một hàm để lúc sau có thể dùng lại cho tập kiểm tra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_new_input_df(new_input_df, dropped_cols, mean_mode_dict, X_mean, X_std):\n",
    "    '''\n",
    "    Builds `new_X` from `new_input_df`:\n",
    "    1. Drop columns using `dropped_cols` (`dropped_cols` is the list containing names of dropped columns)\n",
    "    2. Fill missing values using `mean_mode_dict` (use `fill_missing_values` function)\n",
    "    3. Convert categorial columns to one-hot (use `process_categorical_cols` function)\n",
    "    4. Subtract by `X_mean` and divide by `X_std`\n",
    "    5. Add `x_0` column\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    I'm lazy now ...\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    new_X : numpy array\n",
    "        The matrix of input vectors.\n",
    "    '''\n",
    "    new_input_df = new_input_df.drop(dropped_cols, axis=1)\n",
    "    new_input_df = fill_missing_values(new_input_df, mean_mode_dict)\n",
    "    new_input_df = process_categorical_cols(new_input_df)\n",
    "    new_input_df = preprocessing_Name(new_input_df)\n",
    "    print(new_input_df.info())\n",
    "    new_X = new_input_df.values\n",
    "    new_X = (new_X - X_mean)/X_std\n",
    "#     new_X = np.hstack([np.ones((len(new_X), 1)), new_X])\n",
    "    return new_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 179 entries, 529 to 685\n",
      "Data columns (total 24 columns):\n",
      "Pclass           179 non-null int64\n",
      "Age              179 non-null float64\n",
      "SibSp            179 non-null int64\n",
      "Parch            179 non-null int64\n",
      "female           179 non-null float64\n",
      " the Countess    179 non-null float64\n",
      " Major           179 non-null float64\n",
      " Col             179 non-null float64\n",
      " Ms              179 non-null float64\n",
      " Lady            179 non-null float64\n",
      " Master          179 non-null float64\n",
      " Mme             179 non-null float64\n",
      " Capt            179 non-null float64\n",
      " Miss            179 non-null float64\n",
      " Mrs             179 non-null float64\n",
      " Don             179 non-null float64\n",
      " Mlle            179 non-null float64\n",
      " Dr              179 non-null float64\n",
      " Jonkheer        179 non-null float64\n",
      " Mr              179 non-null float64\n",
      " Rev             179 non-null float64\n",
      "C                179 non-null float64\n",
      "Q                179 non-null float64\n",
      "Fare             179 non-null float64\n",
      "dtypes: float64(21), int64(3)\n",
      "memory usage: 40.0 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "val_X = process_new_input_df(val_input_df, dropped_cols, mean_mode_dict, X_mean, X_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2. Xây dựng `val_Y` từ `val_output_df`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_Y = val_output_df.values.reshape(-1, 1)\n",
    "val_Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>Huấn luyện"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Các activation function thường gặp\n",
    "\n",
    "**Sigmoid**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\sigma(s) = \\frac{1}{1+e^{-s}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**swish**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{swish}(s) = s*\\sigma(s) = \\frac{s}{1+e^{-s}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**tanh**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{tanh}(s) = 2\\sigma(2s)-1 = \\frac{e^s-e^{-s}}{e^s+e^{-s}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**relu**\n",
    "$$\n",
    "\\begin{aligned}\n",
    " \\text{relu}(s) = \\text{max}(0,s)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Note**: Các hàm trên được áp dụng cho một số thực $(s \\in \\mathbb{R})$, khi áp dụng đầu vào là một vector thì các hàm trên được áp dụng trên từng phần tử (element-wise).\n",
    "\n",
    "------------------------------------\n",
    "**softmax**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\sigma (\\mathbf {z} )_{i}={\\frac {e^{\\beta z_{i}}}{\\sum _{j=1}^{K}e^{\\beta z_{j}}}}{\\text{ với }}i=1,\\dotsc ,K\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Sigmoid activation function.\n",
    "        g(z) = 1 / (1 + e^-z)\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def swish(z):\n",
    "    \"\"\"\n",
    "    Swish activation function.\n",
    "        g(z) = z * sigmoid(z) = z * 1 / (1 + e^-z)\n",
    "    \"\"\"\n",
    "    return z * (1/(1+np.exp(-z)))\n",
    "\n",
    "def tanh(z):\n",
    "    \"\"\"\n",
    "    Tanh activation function.\n",
    "        g(z) = tanh(z)\n",
    "    \"\"\"\n",
    "    return np.tanh(z)\n",
    "\n",
    "def relu(z):\n",
    "    \"\"\"\n",
    "    Relu activation function.\n",
    "        g(z) = max(0, z)\n",
    "    \"\"\"\n",
    "    return z*(z > 0)\n",
    "\n",
    "def softmax(z, axis=-1):\n",
    "    \"\"\"\n",
    "    Softmax activation function. Use at the output layer.\n",
    "        g(z) = e^z / sum(e^z)\n",
    "    \"\"\"\n",
    "    z_prime = z - np.max(z, axis=axis, keepdims=True)\n",
    "    return np.exp(z_prime) / np.sum(np.exp(z_prime), axis=axis, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derivative của các Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_grad(z):\n",
    "    \"\"\"\n",
    "    Sigmoid derivative.\n",
    "        g'(z) = g(z)(1-g(z))\n",
    "    \"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "def swish_grad(z):\n",
    "    \"\"\"\n",
    "    Swish derivative.\n",
    "        g'(z) = g(z) + sigmoid(z)(1-g(z))\n",
    "    \"\"\"\n",
    "    return swish(z) + sigmoid(z)*(1-swish(z))\n",
    "\n",
    "def tanh_grad(z):\n",
    "    \"\"\"\n",
    "    Tanh derivative.\n",
    "        g'(z) = 1 - g^2(z).\n",
    "    \"\"\"\n",
    "    return 1 - tanh(z)**2\n",
    "\n",
    "def relu_grad(z):\n",
    "    \"\"\"\n",
    "    Relu derivative.\n",
    "        g'(z) = 0 if g(z) <= 0\n",
    "        g'(z) = 1 if g(z) > 0\n",
    "    \"\"\"\n",
    "    return 1*(relu(z) > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer weight initializers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def he_normal(weight_shape):\n",
    "    \"\"\"\n",
    "    Initialize weights according `He normal` distribution. With mean = 0, std = sqrt(2 / num_input)\n",
    "    \"\"\"\n",
    "    if len(weight_shape) == 4:\n",
    "        fW, fH, fC, _ = weight_shape\n",
    "        return np.random.normal(0, np.sqrt(2 / (fW*fH*fC)), weight_shape)\n",
    "    num_input, _ = weight_shape\n",
    "    return np.random.normal(0, np.sqrt(2 / num_input), weight_shape)\n",
    "\n",
    "def he_uniform(weight_shape):\n",
    "    \"\"\"\n",
    "    Initialize weights according `He uniform` distribution within the range [-limit, limit].\n",
    "                With limit = sqrt(6 / num_input)\n",
    "    \"\"\"\n",
    "    if len(weight_shape) == 4:\n",
    "        fW, fH, fC, _ = weight_shape\n",
    "        return np.random.uniform(-np.sqrt(6 / (fW*fH*fC)), np.sqrt(6 / (fW*fH*fC)), weight_shape)\n",
    "    num_input, _ = weight_shape\n",
    "    return np.random.uniform(-np.sqrt(6 / num_input), np.sqrt(6 / num_input), weight_shape)\n",
    "\n",
    "def glorot_normal(weight_shape):\n",
    "    \"\"\"\n",
    "    Initialize weights according `Xavier normal` distribution. With mean = 0, std = sqrt(2 / (num_input + num_output))\n",
    "    \"\"\"\n",
    "    if len(weight_shape) == 4:\n",
    "        fW, fH, fC, num_fitls = weight_shape\n",
    "        return np.random.normal(0, np.sqrt(2 / (fW*fH*fC + num_fitls)), weight_shape)\n",
    "    num_input, num_output = weight_shape\n",
    "    return np.random.normal(0, np.sqrt(2 / (num_input + num_output)), weight_shape)\n",
    "\n",
    "def glorot_uniform(weight_shape):\n",
    "    \"\"\"\n",
    "    Initialize weights according `Xavier uniform` distribution within the range [-limit, limit].\n",
    "                With limit = sqrt(6 / (num_input + num_output))\n",
    "    \"\"\"\n",
    "    if len(weight_shape) == 4:\n",
    "        fW, fH, fC, num_fitls = weight_shape\n",
    "        return np.random.uniform(-np.sqrt(6 / (fW*fH*fC + num_fitls)), np.sqrt(6 / (fW*fH*fC + num_fitls)), weight_shape)\n",
    "    num_input, num_output = weight_shape\n",
    "    return np.random.uniform(-np.sqrt(6 / (num_input + num_output)), np.sqrt(6 / (num_input + num_output)), weight_shape)\n",
    "\n",
    "def standard_normal(weight_shape):\n",
    "    \"\"\"\n",
    "    Initialize weights according standard normal distribution with mean 0 variance 1.\n",
    "    \"\"\"\n",
    "    return np.random.normal(size=weight_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialization_mapping = {\"glorot_normal\": glorot_normal, \"he_normal\": he_normal, \"glorot_normal\": glorot_normal, \"std\": standard_normal,\n",
    "                          \"he_uniform\": he_uniform, \"glorot_uniform\": glorot_uniform}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layers trong Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "    def forward(self, X):\n",
    "        raise NotImplementedError(\"forward() function not defined\")\n",
    "\n",
    "    def backward(self):\n",
    "        raise NotImplementedError(\"backward() function not defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnableLayer:\n",
    "\n",
    "    def forward(self, X):\n",
    "        raise NotImplementedError(\"forward() function not defined\")\n",
    "\n",
    "    def backward_layer(self):\n",
    "        pass\n",
    "\n",
    "    def backward(self):\n",
    "        raise NotImplementedError(\"backward() function not defined\")\n",
    "\n",
    "    def update_params(self, grad_W, grad_b):\n",
    "        self.W = self.W - grad_W\n",
    "        self.bias = self.bias - grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Input(Layer):\n",
    "\n",
    "    def __init__(self, return_dX=False):\n",
    "        self.return_dX = return_dX\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.output = X\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d_prev, weights_prev):\n",
    "        \"\"\"\n",
    "        d_prev: gradient of J respect to A[l+1] of the previous layer according backward direction.\n",
    "        weights_prev: the weights of previous layer according backward direction.\n",
    "        \"\"\"\n",
    "        if self.return_dX:\n",
    "            return np.dot(d_prev, weights_prev.T)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(LearnableLayer):\n",
    "\n",
    "    def __init__(self, num_neurons, weight_init=\"std\"):\n",
    "        \"\"\"\n",
    "        The fully connected layer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_neurons: (integer) number of neurons in the layer.     \n",
    "        weight_init: (string) either `he_normal`, `xavier_normal`, `he_uniform`, `glorot_uniform` or standard normal distribution.\n",
    "        \"\"\"\n",
    "        assert weight_init in [\"std\", \"glorot_normal\", \"he_normal\", \"he_uniform\", \"glorot_uniform\"],\\\n",
    "                \"Unknow weight initialization type.\"\n",
    "        self.num_neurons = num_neurons\n",
    "        self.weight_init = weight_init\n",
    "        self.output = None\n",
    "        self.W = None\n",
    "        self.bias = None\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Layer forward level. \n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs: inputs of the current layer. This is equivalent to the output of the previous layer.\n",
    "        Returns\n",
    "        -------\n",
    "        output: Output value LINEAR of the current layer.\n",
    "        \"\"\"\n",
    "        if self.W is None:\n",
    "            self.W = initialization_mapping[self.weight_init](weight_shape=(inputs.shape[1], self.num_neurons))\n",
    "    \n",
    "        if self.bias is None:\n",
    "            self.bias = initialization_mapping[self.weight_init](weight_shape=(1, self.num_neurons))\n",
    "        \n",
    "        self.output = np.dot(inputs, self.W) + self.bias\n",
    "        return self.output\n",
    "\n",
    "    def backward_layer(self, d_prev, _):\n",
    "        \"\"\"\n",
    "        Compute gradient w.r.t X only.\n",
    "        \"\"\"\n",
    "        d_prev = np.dot(d_prev, self.W.T)\n",
    "        return d_prev\n",
    "\n",
    "    def backward(self, d_prev, prev_layer):\n",
    "        \"\"\"\n",
    "        Layer backward level. Compute gradient respect to W and update it.\n",
    "        Also compute gradient respect to X for computing gradient of previous\n",
    "        layers as the forward direction [l-1].\n",
    "        Parameters\n",
    "        ----------\n",
    "        d_prev: gradient of J respect to A[l+1] of the previous layer according backward direction.\n",
    "        prev_layer: previous layer according forward direction.\n",
    "        Returns\n",
    "        -------\n",
    "        d_prev: gradient of J respect to A[l] at the current layer.\n",
    "        \"\"\"\n",
    "        dW = np.dot(prev_layer.output.T, d_prev)\n",
    "        db = np.sum(d_prev,axis=0)\n",
    "        d_prev = self.backward_layer(d_prev, None)\n",
    "        return d_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(Layer):\n",
    "\n",
    "    \"\"\"\n",
    "    Refer to the paper: \n",
    "        http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, keep_prob):\n",
    "        \"\"\"\n",
    "        keep_prob: (float) probability to keep neurons in network, use for dropout technique.\n",
    "        \"\"\"\n",
    "        assert 0.0 < keep_prob < 1.0, \"keep_prob must be in range [0, 1].\"\n",
    "        self.keep_prob = keep_prob\n",
    "\n",
    "    def forward(self, X, prediction=False):\n",
    "        \"\"\"\n",
    "        Drop neurons random uniformly.\n",
    "        \"\"\"\n",
    "        if prediction:\n",
    "            self.output = X * self.keep_prob\n",
    "            return self.output\n",
    "        \n",
    "        self.mask = np.random.uniform(size=X.shape) < self.keep_prob\n",
    "        self.output = X * self.mask\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d_prev, _):\n",
    "        \"\"\"\n",
    "        Flow gradient of previous layer [l+1] according backward direction through dropout layer.\n",
    "        \"\"\"\n",
    "        return d_prev * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(Layer):\n",
    "\n",
    "    def __init__(self, activation):\n",
    "        \"\"\"\n",
    "        activation: (string) available activation functions. Must be in [sigmoid, tanh,\n",
    "                                relu, softmax]. Softmax activation must be at the last layer.\n",
    "        \n",
    "        \"\"\"\n",
    "        assert activation in [\"swish\", \"sigmoid\", \"tanh\", \"relu\", \"softmax\"], \"Unknown activation function: \" + str(activation)\n",
    "        self.activation = activation\n",
    "        self.last = False\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Activation layer forward propgation.\n",
    "        \"\"\"\n",
    "        self.output = eval(self.activation)(X)\n",
    "        self.input = X\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d_prev, _):\n",
    "        \"\"\"\n",
    "        Activation layer backward propagation.\n",
    "        Parameters\n",
    "        ---------- \n",
    "        d_prev: gradient of J respect to A[l+1] of the previous layer according backward direction.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Gradient of J respect to type of activations (sigmoid, tanh, relu) in this layer `l`.\n",
    "        \"\"\"\n",
    "        if self.last:\n",
    "            # return previous derivatives of loss, because we computed derivatives of softmax with CE-loss already.\n",
    "            # ref: https://math.stackexchange.com/questions/945871/derivative-of-softmax-loss-function\n",
    "            return d_prev\n",
    "        d_prev = d_prev * eval(self.activation + \"_grad\")(self.input)\n",
    "        return d_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy:\n",
    "    \n",
    "    def __init__(self, weights=1, epsilon=1e-20):\n",
    "        self.weights = 1\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def __call__(self, Y_hat, Y):\n",
    "        \"\"\"\n",
    "        Compute cross-entropy loss.\n",
    "        Parameters\n",
    "        ----------\n",
    "        Y: one-hot encoding label. shape=(num_dataset, num_classes)\n",
    "        Y_hat: softmax probability distribution over each data point. \n",
    "            shape=(num_dataset, num_classes)\n",
    "        Returns\n",
    "        -------\n",
    "        J: cross-entropy loss.\n",
    "        \"\"\"\n",
    "        assert Y.shape == Y_hat.shape, \"Unmatch shape.\"\n",
    "        Y_hat[Y_hat == 0] = self.epsilon\n",
    "        loss = np.sum(self.weights * Y * np.log(Y_hat), axis=-1)\n",
    "        return -np.mean(loss)\n",
    "\n",
    "    def backward(self, Y_hat, Y):\n",
    "        \"\"\"\n",
    "        Compute gradient of CE w.r.t linear (LINEAR -> SOFTMAX -> CE)\n",
    "        Parameters\n",
    "        ----------\n",
    "        Y: one-hot encoding label. shape=(num_dataset, num_classes)\n",
    "        Y_hat: softmax probability distribution over each data point. \n",
    "            shape=(num_dataset, num_classes)\n",
    "        Returns\n",
    "        -------\n",
    "        grad CE w.r.t LINEAR\n",
    "        \"\"\"\n",
    "        m = Y.shape[0]\n",
    "        return (Y_hat - Y)/m\n",
    "\n",
    "class MSE:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, y_hat, y):\n",
    "        \"\"\"\n",
    "        Mean squared error\n",
    "        \"\"\"\n",
    "        m = len(y)\n",
    "        loss = np.sum((y_hat - y)**2)/(2*m)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, y_hat, y):\n",
    "        \"\"\"\n",
    "        Compute gradient of MSE w.r.t y_hat\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_hat: output from linear transformation. shape = (num_dataset, )\n",
    "        y: ground truth, real values. shape = (num_dataset, )\n",
    "        \"\"\"\n",
    "        m = len(y)\n",
    "        grad = (y_hat - y)/m\n",
    "        return grad\n",
    "\n",
    "class BinaryCrossEntropy:\n",
    "    \n",
    "    def __init__(self, epsilon=1e-10):\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def __call__(self, y_hat, y):\n",
    "        m = len(y_hat)\n",
    "        y_hat[y_hat == 0] = self.epsilon\n",
    "        y_hat[y_hat == 1] = 1 - self.epsilon\n",
    "        loss = -np.mean(y*np.log(y_hat) + (1-y)*np.log(1 - y_hat))\n",
    "        return loss\n",
    "\n",
    "    def backward(self, y_hat, y):\n",
    "        m = len(y)\n",
    "        grad = (y_hat - y)/m\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizers_: \n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def step(self, grads, layers):\n",
    "        raise NotImplementedError(\"step() function not defined\")\n",
    "\n",
    "class SGD(Optimizers_):\n",
    "\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self, grads_W, grads_b, layers):\n",
    "        for grad_W, grad_b, layer in zip(grads_W, grads_b, layers):\n",
    "            grad_W = self.lr * grad_W\n",
    "            grad_b = self.lr * grad_b\n",
    "            layer.update_params(grad_W, grad_b)\n",
    "\n",
    "class SGDMomentum(Optimizers_):\n",
    "\n",
    "    def __init__(self, alpha=0.01, beta=0.9):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.vW = []\n",
    "        self.vb = []\n",
    "    \n",
    "    def step(self, grads_W, grads_b, layers):\n",
    "        if len(self.vW) == 0 and len(self.vb) == 0:\n",
    "            self.vW = [np.zeros_like(grad) for grad in grads_W]\n",
    "            self.vb = [np.zeros_like(grad) for grad in grads_b]\n",
    "        \n",
    "        for i, (grad_W, grad_b, layer) in enumerate(zip(grads_W, grads_b, layers)):\n",
    "            self.vW[i] = self.beta*self.vW[i] + (1-self.beta)*grad_W\n",
    "            grad_W = self.alpha * self.vW[i]\n",
    "            self.vb[i] = self.beta*self.vb[i] + (1-self.beta)*grad_b\n",
    "            grad_b = self.alpha * self.vb[i]\n",
    "            layer.update_params(grad_W, grad_b)\n",
    "\n",
    "class RMSProp(Optimizers_):\n",
    "\n",
    "    def __init__(self, alpha=0.01, beta=0.9, epsilon=1e-9):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.epsilon = epsilon\n",
    "        self.sW = []\n",
    "        self.sb = []\n",
    "\n",
    "    def step(self, grads_W, grads_b, layers):\n",
    "        if len(self.sW) == 0 and len(self.sb) == 0:\n",
    "            self.sW = [np.zeros_like(grad) for grad in grads_W]\n",
    "            self.sb = [np.zeros_like(grad) for grad in grads_b]\n",
    "        for i, (grad_W, grad_b, layer) in enumerate(zip(grads_W, grads_b, layers)):\n",
    "            self.sW[i] = self.beta*self.sW[i] + (1-self.beta)*grad_W**2\n",
    "            grad_W = self.alpha * (grad_W/(np.sqrt(self.sW[i]) + self.epsilon))\n",
    "            self.sb[i] = self.beta*self.sb[i] + (1-self.beta)*grad_b**2\n",
    "            grad_b = self.alpha * (grad_b/(np.sqrt(self.sb[i]) + self.epsilon))\n",
    "            layer.update_params(grad_W, grad_b)\n",
    "\n",
    "class Adam(Optimizers_):\n",
    "    \n",
    "    def __init__(self, alpha=0.01, beta_1=0.9, beta_2=0.99, epsilon=1e-9):\n",
    "        self.alpha = alpha\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        self.epsilon = epsilon\n",
    "        self.vW = []\n",
    "        self.sW = []\n",
    "        self.vb = []\n",
    "        self.sb = []\n",
    "        self.t = 1\n",
    "\n",
    "    def step(self, grads_W, grads_b, layers):\n",
    "        if len(self.sW) == 0 and len(self.vW) == 0 and len(self.sb) == 0 and len(self.vb) == 0:\n",
    "            self.vW = [np.zeros_like(grad) for grad in grads_W]\n",
    "            self.sW = [np.zeros_like(grad) for grad in grads_W]\n",
    "            self.vb = [np.zeros_like(grad) for grad in grads_b]\n",
    "            self.sb = [np.zeros_like(grad) for grad in grads_b]\n",
    "            \n",
    "        for i, (grad_W, grad_b, layer) in enumerate(zip(grads_W, grads_b, layers)):\n",
    "            self.vW[i] = (self.beta_1*self.vW[i] + (1-self.beta_1)*grad_W)\n",
    "            self.sW[i] = (self.beta_2*self.sW[i] + (1-self.beta_2)*grad_W**2)\n",
    "            v_correct = self.vW[i] / (1-self.beta_1**self.t)\n",
    "            s_correct = self.sW[i] / (1-self.beta_2**self.t)\n",
    "            grad_W = self.alpha * (v_correct / (np.sqrt(s_correct) + self.epsilon))\n",
    "            \n",
    "            self.vb[i] = (self.beta_1*self.vb[i] + (1-self.beta_1)*grad_b)\n",
    "            self.sb[i] = (self.beta_2*self.sb[i] + (1-self.beta_2)*grad_b**2)\n",
    "            v_correct = self.vb[i] / (1-self.beta_1**self.t)\n",
    "            s_correct = self.sb[i] / (1-self.beta_2**self.t)\n",
    "            grad_b = self.alpha * (v_correct / (np.sqrt(s_correct) + self.epsilon))\n",
    "            \n",
    "            layer.update_params(grad_W, grad_b)\n",
    "        self.t += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "\n",
    "    def __init__(self, optimizer:object, layers:list, loss_func:object=CrossEntropy()):\n",
    "        \"\"\"\n",
    "        Deep neural network architecture.\n",
    "        Parameters\n",
    "        ----------\n",
    "        optimizer: (object) optimizer object uses to optimize the loss.\n",
    "        layers: (list) a list of sequential layers. For neural network, it should have [Dense, Activation, BatchnormLayer, Dropout]\n",
    "        loss_func: (object) the type of loss function we want to optimize. \n",
    "        \"\"\"\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_func = loss_func\n",
    "        self.layers = layers\n",
    "        if isinstance(self.layers[-1], Activation):\n",
    "            self.layers[-1].last = True\n",
    "\n",
    "    def _forward(self, train_X, prediction=False):\n",
    "        \"\"\"\n",
    "        NN forward propagation level.\n",
    "        Parameters\n",
    "        ----------\n",
    "        train_X: training dataset X.\n",
    "                shape = (N, D)\n",
    "        prediction: whether this forward pass is prediction stage or training stage.\n",
    "        Returns\n",
    "        -------\n",
    "        Probability distribution of softmax at the last layer.\n",
    "            shape = (N, C)\n",
    "        \"\"\"\n",
    "        inputs = train_X\n",
    "        layers = self.layers\n",
    "            \n",
    "        for layer in layers:\n",
    "            if isinstance(layer, Dropout):\n",
    "                inputs = layer.forward(inputs, prediction=prediction)\n",
    "                continue\n",
    "            inputs = layer.forward(inputs)\n",
    "        output = inputs\n",
    "        return output\n",
    "    \n",
    "    def __call__(self, X, prediction=False):\n",
    "        return self._forward(X, prediction)\n",
    "\n",
    "    def _update_params(self, grads_W, grads_b):\n",
    "        self.optimizer.step(grads_W, grads_b, self.learnable_layers)\n",
    "\n",
    "    def backward(self, Y, Y_hat, X):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        Y: one-hot encoding label.\n",
    "            shape = (N, C).\n",
    "        Y_hat: output values of forward propagation NN.\n",
    "            shape = (N, C).\n",
    "        X: training dataset.\n",
    "            shape = (N, D).\n",
    "        \"\"\"\n",
    "        if not hasattr(self, \"output_layers\"):\n",
    "            self.learnable_layers = [layer for layer in self.layers if isinstance(layer, LearnableLayer)]\n",
    "            self.learnable_layers = self.learnable_layers[::-1]\n",
    "\n",
    "        grads_W, grads_b = [], []\n",
    "        \n",
    "        dCost = self.loss_func.backward(Y_hat, Y)\n",
    "        dA_prev, dW = dCost, None\n",
    "        \n",
    "        for i in range(len(self.layers)-1, 0, -1):\n",
    "            if isinstance(self.layers[i], LearnableLayer):\n",
    "                dA_prev, dW, db = self.layers[i].backward(dA_prev, self.layers[i-1])\n",
    "                grads_W.append(dW)\n",
    "                grads_b.append(db)\n",
    "                continue\n",
    "            dA_prev = self.layers[i].backward(dA_prev, self.layers[i-1])\n",
    "\n",
    "        self._update_params(grads_W, grads_b)\n",
    "\n",
    "    def predict(self, test_X):\n",
    "        \"\"\"\n",
    "        Predict function.\n",
    "        \"\"\"\n",
    "        y_hat = self._forward(test_X, prediction=True)\n",
    "        return np.argmax(y_hat, axis=1)\n",
    "    \n",
    "    def cal_accuracy(self, Y, pred):\n",
    "        if len(Y.shape) > 1:\n",
    "            Y = enc.inverse_transform(Y).reshape(Y.shape[0])\n",
    "        else:\n",
    "            Y = Y.reshape(Y.shape[0])\n",
    "        return len(pred[Y == pred]) / len(pred)\n",
    "    \n",
    "    \n",
    "    def fit(self, X_train, y_train, validation, batch_size, epochs):\n",
    "        m = X_train.shape[0]\n",
    "        X_val, y_val = validation\n",
    "        train_losses, val_losses = [],[]\n",
    "        train_accs, val_accs = [],[]\n",
    "        \n",
    "        \n",
    "        for e in range(epochs):\n",
    "            indices = np.random.permutation(m)\n",
    "            X_train = X_train[indices]\n",
    "            y_train = y_train[indices]\n",
    "            epoch_loss, val_loss = 0.0, 0.0\n",
    "            num_batches, val_batches = 0, 0\n",
    "            pbar = tqdm(range(0, X_train.shape[0], batch_size))\n",
    "            \n",
    "            for it in pbar:\n",
    "                X_batch = X_train[it:it+batch_size]\n",
    "                y_batch = y_train[it:it+batch_size]\n",
    "                \n",
    "                y_hat = self._forward(X_batch, prediction=False)\n",
    "                batch_loss = self.loss_func(y_hat, y_batch)\n",
    "                self.backward(y_batch, y_hat, X_batch)\n",
    "\n",
    "                epoch_loss += batch_loss\n",
    "                num_batches += 1\n",
    "                pbar.set_description(\"Epoch \" + str(e+1) + \" - Loss: %.5f\" % (epoch_loss/num_batches))\n",
    "                \n",
    "            for it in range(0, X_val.shape[0], batch_size):\n",
    "                X_batch = X_val[it:it+batch_size]\n",
    "                y_batch = y_val[it:it+batch_size]\n",
    "                \n",
    "                y_val_hat = self._forward(X_batch)\n",
    "                batch_val_loss = self.loss_func(y_val_hat, y_batch)\n",
    "                val_loss += batch_val_loss\n",
    "                val_batches += 1\n",
    "            \n",
    "            train_losses.append(epoch_loss/num_batches)\n",
    "            val_losses.append(val_loss/val_batches)\n",
    "            \n",
    "            train_acc = self.cal_accuracy(y_train, self.predict(X_train))\n",
    "            val_acc = self.cal_accuracy(y_val, self.predict(X_val))\n",
    "            train_accs.append(train_acc)\n",
    "            val_accs.append(val_acc)\n",
    "            pbar.close()\n",
    "            print(\"Loss at epoch %d: %.5f - Train accuracy: %.5f - Validation loss: %.5f - accuracy: %.5f\" % (e+1, epoch_loss/num_batches, train_acc, val_loss/val_batches, val_acc))\n",
    "        return train_losses, val_losses, train_accs, val_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cấu hình Neural Net và Huấn luyện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b128d9748e72456eb127626f863fe66b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss at epoch 1: 1.35224 - Train accuracy: 0.36657 - Validation loss: 1.02123 - accuracy: 0.45251\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e19c8c651d946afb6fffaf0c5743b52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss at epoch 2: 0.95582 - Train accuracy: 0.36657 - Validation loss: 0.70186 - accuracy: 0.45251\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bbba7c9cc3b4f34848050961139c5e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss at epoch 3: 0.64375 - Train accuracy: 0.77809 - Validation loss: 0.49064 - accuracy: 0.80447\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94a64879edd54bd2b44f71439bc592f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss at epoch 4: 0.48770 - Train accuracy: 0.80056 - Validation loss: 0.43523 - accuracy: 0.83240\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12453015df3a494fa59adfd1d6d8c5d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss at epoch 5: 0.43881 - Train accuracy: 0.80899 - Validation loss: 0.41092 - accuracy: 0.83240\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54962bf63e3f440389e7304952e57853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss at epoch 6: 0.43574 - Train accuracy: 0.81742 - Validation loss: 0.41001 - accuracy: 0.83240\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52a7920fd5d147ecbc08320c32140d61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss at epoch 7: 0.42319 - Train accuracy: 0.81882 - Validation loss: 0.40831 - accuracy: 0.84358\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d53fb9b4046c48c3b85299aa44549fff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss at epoch 8: 0.42611 - Train accuracy: 0.82303 - Validation loss: 0.40255 - accuracy: 0.84358\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32ba96df91ff40b29de28e98882e00a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss at epoch 9: 0.41612 - Train accuracy: 0.82584 - Validation loss: 0.39915 - accuracy: 0.84358\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efe2091cad56408197ca12610bd3354b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss at epoch 10: 0.41823 - Train accuracy: 0.83006 - Validation loss: 0.39865 - accuracy: 0.83799\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3724b1c1b0554adda2f2da596f1a73ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss at epoch 11: 0.41084 - Train accuracy: 0.83287 - Validation loss: 0.39666 - accuracy: 0.83799\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bad632c7c6f444d2b64c8eb49a23d609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss at epoch 12: 0.41311 - Train accuracy: 0.83427 - Validation loss: 0.39236 - accuracy: 0.83799\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63be0f09d3084124ad0140d38e261ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss at epoch 13: 0.41274 - Train accuracy: 0.83708 - Validation loss: 0.40586 - accuracy: 0.83240\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6296894d45f04013833c629244eafecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss at epoch 14: 0.41037 - Train accuracy: 0.83427 - Validation loss: 0.39218 - accuracy: 0.83799\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a765930ec346778df6c38e26bcf90c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss at epoch 15: 0.40786 - Train accuracy: 0.83427 - Validation loss: 0.39364 - accuracy: 0.83799\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "875bcf481a34476bafdfe5c5397b9d5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss at epoch 16: 0.41070 - Train accuracy: 0.83427 - Validation loss: 0.38888 - accuracy: 0.83799\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f5114170fe44c2499496c75771cdcde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss at epoch 17: 0.40832 - Train accuracy: 0.83567 - Validation loss: 0.39224 - accuracy: 0.83799\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce38faf3bd994852a7afb3e56575ca46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss at epoch 18: 0.40503 - Train accuracy: 0.83427 - Validation loss: 0.40577 - accuracy: 0.83799\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ace22d6d1f9b40c0ae24acd9ce10a30e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss at epoch 19: 0.41158 - Train accuracy: 0.83427 - Validation loss: 0.39258 - accuracy: 0.83799\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feb6d10f08a44bdd9f0db1a42add2253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss at epoch 20: 0.40466 - Train accuracy: 0.83287 - Validation loss: 0.39117 - accuracy: 0.83799\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "870640bfbff14507868f0641b185fb79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss at epoch 21: 0.40520 - Train accuracy: 0.83567 - Validation loss: 0.39288 - accuracy: 0.82123\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c9287309f904170b1ba1a56a5736fc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss at epoch 22: 0.40383 - Train accuracy: 0.83848 - Validation loss: 0.40525 - accuracy: 0.82123\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "823a1d3dde1d45debf8e2b9f52d79408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss at epoch 23: 0.41522 - Train accuracy: 0.83567 - Validation loss: 0.39539 - accuracy: 0.82682\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cb025efac27422594db029fc39cc5a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss at epoch 24: 0.40347 - Train accuracy: 0.83848 - Validation loss: 0.38962 - accuracy: 0.83240\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36a372db503543ce9f3760e9f4e8d289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss at epoch 25: 0.40371 - Train accuracy: 0.83708 - Validation loss: 0.37601 - accuracy: 0.82682\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8f0d83d75594a70afd3c5da4f6dff93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss at epoch 26: 0.40658 - Train accuracy: 0.83567 - Validation loss: 0.38815 - accuracy: 0.83799\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c56b8ff84874b66b6d38fde765b8199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss at epoch 27: 0.40255 - Train accuracy: 0.83287 - Validation loss: 0.38735 - accuracy: 0.82682\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80b8cb8129344b9180eaecc0a1794022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss at epoch 28: 0.40796 - Train accuracy: 0.83989 - Validation loss: 0.39108 - accuracy: 0.82682\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7dad115c05c471ba5e431dc766ac381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss at epoch 29: 0.41001 - Train accuracy: 0.83567 - Validation loss: 0.38549 - accuracy: 0.81564\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5103b238f1694353b832a1a683b5a6e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss at epoch 30: 0.40563 - Train accuracy: 0.82865 - Validation loss: 0.38955 - accuracy: 0.81564\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc7c3a93e4ae49b98277b0ab4238bfcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss at epoch 31: 0.40481 - Train accuracy: 0.83006 - Validation loss: 0.39092 - accuracy: 0.81564\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2931f8020e04c72bace344cad79e666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss at epoch 32: 0.40459 - Train accuracy: 0.83146 - Validation loss: 0.38922 - accuracy: 0.82123\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4175f7aecc7743d891d82fef83ae81b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss at epoch 33: 0.40685 - Train accuracy: 0.84129 - Validation loss: 0.39427 - accuracy: 0.82682\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9c3168b6b364c2ca9272514859d53b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss at epoch 34: 0.41221 - Train accuracy: 0.82865 - Validation loss: 0.38721 - accuracy: 0.82123\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b6ad282f0884fa18151a166391899de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss at epoch 35: 0.40845 - Train accuracy: 0.83287 - Validation loss: 0.39227 - accuracy: 0.81564\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35ab0989e7ce4bcb957f36d6a4f578ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss at epoch 36: 0.40206 - Train accuracy: 0.83989 - Validation loss: 0.39271 - accuracy: 0.82682\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04dcdaf2424048238bccce0c2caab97a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss at epoch 37: 0.40673 - Train accuracy: 0.83848 - Validation loss: 0.39586 - accuracy: 0.82123\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b77209da9f41dabc2674c836abb694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss at epoch 38: 0.41031 - Train accuracy: 0.83989 - Validation loss: 0.39263 - accuracy: 0.82123\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b0bd479db8640c59cfc68adfc1baf90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss at epoch 39: 0.40111 - Train accuracy: 0.83848 - Validation loss: 0.38738 - accuracy: 0.83240\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b8f836c53ed4f0a901f9dc83b62f9c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss at epoch 40: 0.40338 - Train accuracy: 0.83006 - Validation loss: 0.38288 - accuracy: 0.81564\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "371d3319b2cd49cd90d63c758833e6e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss at epoch 41: 0.40243 - Train accuracy: 0.83427 - Validation loss: 0.38718 - accuracy: 0.81564\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91ee3eee39934818960c5ab68e1c6902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss at epoch 42: 0.40443 - Train accuracy: 0.83989 - Validation loss: 0.39638 - accuracy: 0.82123\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1d6469622da41ada655250edc3c97c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss at epoch 43: 0.40236 - Train accuracy: 0.83848 - Validation loss: 0.39463 - accuracy: 0.82682\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5d4a60e1d204726a7d4e30cc80b6ad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss at epoch 44: 0.40538 - Train accuracy: 0.83989 - Validation loss: 0.38555 - accuracy: 0.82123\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26bc49bb32b34aa69a6566be101b92bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss at epoch 45: 0.40229 - Train accuracy: 0.83427 - Validation loss: 0.38845 - accuracy: 0.81564\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be781052ba5f41e1af5c7ab5bd5a7ab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss at epoch 46: 0.39984 - Train accuracy: 0.83287 - Validation loss: 0.39894 - accuracy: 0.81564\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a199bda7ed434ebaadfc534060e17271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss at epoch 47: 0.39744 - Train accuracy: 0.83708 - Validation loss: 0.39456 - accuracy: 0.82123\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4588b7bf0084136b750ebd92cd3aee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss at epoch 48: 0.40130 - Train accuracy: 0.83848 - Validation loss: 0.38884 - accuracy: 0.83240\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29602c4e041e42d19e9388dd187fbc20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss at epoch 49: 0.40087 - Train accuracy: 0.83989 - Validation loss: 0.39210 - accuracy: 0.82123\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a681dac9df44b3f8874bda1b7ff7d20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss at epoch 50: 0.39870 - Train accuracy: 0.83848 - Validation loss: 0.38765 - accuracy: 0.82123\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "optimizer = Adam(0.0001)\n",
    "loss_func = BinaryCrossEntropy()\n",
    "archs = [\n",
    "    Input(),\n",
    "    Dense(num_neurons=1024, weight_init=\"glorot_normal\"),\n",
    "    Activation(activation=\"swish\"),\n",
    "    Dropout(keep_prob=0.5),\n",
    "    Dense(num_neurons=512, weight_init=\"glorot_normal\"),\n",
    "    Activation(activation=\"swish\"),\n",
    "    Dropout(keep_prob=0.5),\n",
    "    Dense(num_neurons=128, weight_init=\"glorot_normal\"),\n",
    "    Activation(activation=\"sigmoid\"),\n",
    "    Dense(num_neurons=2, weight_init=\"glorot_normal\"),\n",
    "    Activation(activation=\"softmax\"),\n",
    "]\n",
    "\n",
    "val_set = (val_X, enc.transform(val_Y).toarray())\n",
    "\n",
    "model = Model(optimizer=optimizer, layers=archs, loss_func=loss_func)\n",
    "train_losses, val_losses, train_accs, val_accs = model.fit(train_X, train_Y, val_set, batch_size=16, epochs=EPOCHS)\n",
    "\n",
    "import pickle\n",
    "with open(\"nn_weights.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(EPOCHS), train_losses, label='training')\n",
    "plt.plot(range(EPOCHS), val_losses, label='validation', color='orange')\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(EPOCHS), train_accs, label='training')\n",
    "plt.plot(range(EPOCHS), val_accs, label='validation', color='orange')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8212290502793296\n"
     ]
    }
   ],
   "source": [
    "with open(\"nn_weights.pkl\", \"rb\") as f:\n",
    "            model = pickle.load(f)\n",
    "pred = model.predict(val_X)\n",
    "# val_Y = enc.inverse_transform(val_Y).reshape(val_Y.shape[0])\n",
    "print(\"Accuracy:\", len(pred[val_Y.reshape(val_Y.shape[0]) == pred]) / len(pred))\n",
    "from sklearn.metrics import confusion_matrix\n",
    "array = confusion_matrix(val_Y.reshape(val_Y.shape[0]), pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(cm           = array,\n",
    "                      normalize    = False,\n",
    "                      target_names = ['0', '1'],\n",
    "                      title        = \"Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Kiểm tra (test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Đọc dữ liệu tập kiểm tra vào data frame `test_input_df`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 418 entries, 892 to 1309\n",
      "Data columns (total 10 columns):\n",
      "Pclass      418 non-null int64\n",
      "Name        418 non-null object\n",
      "Sex         418 non-null object\n",
      "Age         332 non-null float64\n",
      "SibSp       418 non-null int64\n",
      "Parch       418 non-null int64\n",
      "Ticket      418 non-null object\n",
      "Fare        417 non-null float64\n",
      "Cabin       91 non-null object\n",
      "Embarked    418 non-null object\n",
      "dtypes: float64(2), int64(3), object(5)\n",
      "memory usage: 35.9+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>892</td>\n",
       "      <td>3</td>\n",
       "      <td>Kelly, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330911</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>893</td>\n",
       "      <td>3</td>\n",
       "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
       "      <td>female</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>363272</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>894</td>\n",
       "      <td>2</td>\n",
       "      <td>Myles, Mr. Thomas Francis</td>\n",
       "      <td>male</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>240276</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>895</td>\n",
       "      <td>3</td>\n",
       "      <td>Wirz, Mr. Albert</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315154</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>896</td>\n",
       "      <td>3</td>\n",
       "      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n",
       "      <td>female</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3101298</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Pclass                                          Name     Sex  \\\n",
       "PassengerId                                                                 \n",
       "892               3                              Kelly, Mr. James    male   \n",
       "893               3              Wilkes, Mrs. James (Ellen Needs)  female   \n",
       "894               2                     Myles, Mr. Thomas Francis    male   \n",
       "895               3                              Wirz, Mr. Albert    male   \n",
       "896               3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female   \n",
       "\n",
       "              Age  SibSp  Parch   Ticket     Fare Cabin Embarked  \n",
       "PassengerId                                                       \n",
       "892          34.5      0      0   330911   7.8292   NaN        Q  \n",
       "893          47.0      1      0   363272   7.0000   NaN        S  \n",
       "894          62.0      0      0   240276   9.6875   NaN        Q  \n",
       "895          27.0      0      0   315154   8.6625   NaN        S  \n",
       "896          22.0      1      1  3101298  12.2875   NaN        S  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input_df = pd.read_csv('test.csv', index_col=0)\n",
    "test_input_df.info()\n",
    "test_input_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Xây dựng `test_X` từ `test_input_df`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "418\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 418 entries, 892 to 1309\n",
      "Data columns (total 24 columns):\n",
      "Pclass           418 non-null int64\n",
      "Age              418 non-null float64\n",
      "SibSp            418 non-null int64\n",
      "Parch            418 non-null int64\n",
      "female           418 non-null float64\n",
      " the Countess    418 non-null float64\n",
      " Major           418 non-null float64\n",
      " Col             418 non-null float64\n",
      " Ms              418 non-null float64\n",
      " Lady            418 non-null float64\n",
      " Master          418 non-null float64\n",
      " Mme             418 non-null float64\n",
      " Capt            418 non-null float64\n",
      " Miss            418 non-null float64\n",
      " Mrs             418 non-null float64\n",
      " Don             418 non-null float64\n",
      " Mlle            418 non-null float64\n",
      " Dr              418 non-null float64\n",
      " Jonkheer        418 non-null float64\n",
      " Mr              418 non-null float64\n",
      " Rev             418 non-null float64\n",
      "C                418 non-null float64\n",
      "Q                418 non-null float64\n",
      "Fare             418 non-null float64\n",
      "dtypes: float64(21), int64(3)\n",
      "memory usage: 101.6 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "test_X = process_new_input_df(test_input_df, dropped_cols, mean_mode_dict, X_mean, X_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Dự đoán nhãn lớp của test_X**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"nn_weights.pkl\", \"rb\") as f:\n",
    "            nn = pickle.load(f)\n",
    "        \n",
    "# Predict\n",
    "preds = nn.predict(test_X)\n",
    "preds_df = pd.DataFrame(preds, index=test_input_df.index, columns=['Survived'])\n",
    "preds_df.head()\n",
    "preds_df.to_csv('preds.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "*submit file `preds.csv` lên [Kaggle](https://www.kaggle.com/c/titanic/submissions/attach), và ghi nhận lại độ chính xác.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kết quả cho single model MLP from Scratch: ```Your submission scored 0.80622```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "153px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "175.188px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
